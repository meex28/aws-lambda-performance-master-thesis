\section{Wyniki przeglądu}\label{chapter:przeglad_literatury_wyniki}

W ramach przeglądu wybrano X prac badawczych. Na bazie prac rozpatrzono postawione pytania badawcze. Odpowiedzi na nie zostały zawarte w kolejnych podrozdziałach.

\input{chapters/przeglad_literatury/results_papers.tex}

\subsection{Czynniki wpływające na wydajność funkcji}\label{chapter:przeglad_literatury_wyniki_czynniki}

Pierwszym pytaniem badawczym postawionym do przeglądu literatury jest: ,,Jakie są główne czynniki wpływające na wydajność funkcji AWS Lambda?''.
Identyfikacja czynników wpływających na wydajność jest kluczowa w kontekście jej optymalizacji.
Pozwoli to następnie na zrozumienie na które z czynników ma także wpływ twórca funkcji AWS Lambda, co ułatwi dalszą analizę metod poprawy ich wydajności.

\subsubsection*{Wielkosć pamięci funkcji}

Kelly, Glavin i Barrett \cite{9284261} zauważają, że wielkość pamięci funkcji oprócz bezpośredniego wpływu na całkowity czas działania, wywiera także wpływ na inne czynniki jak użycie procesora czy wydajność I/O dysku. 
W ramach badania wykonano pomiary dla funkcji bezserwerowych oferowanych przez wielu dostawców chmurowych, w tym Amazon Web Services, w celu zrozumienia infrastruktury i jej zarządzania, co domyślnie jest ukryte dla użytkownika.   
Poprzez analizę maszyn wirtualnych, w ramach których uruchamiany jest kod funkcji, możliwe było otrzymanie wartości parametrów, które nie są domyślnie konfigurowalne podczas wdrażania funkcji przez programistę.

Pomiary pokrywały wiele parametrów funkcji, m. in. łączny czas wykonania, czas inicjalizacji, użycie procesora, wydajność I/O dysku oraz liczbę utworzonych maszyn wirtualnych (co wpływa na częstość zimnych startów). 
W badaniu uwzględniono predefiniowane wielkości pamięci, które mogą być wybrane przez programistę (128MB, 256MB, 512MB, 1024MB oraz 2048MB).
Wraz z wzrostem pamięci funkcji, parametry te poprawiały się.

Autorzy podkreślili ważność odpowiedniego doboru wielkości pamięci podczas tworzenia funkcji. 
Dodatkowo, wykazali, że w przypadku kolejnych wywołań funkcji, platforma AWS ogranicza ponowne użycie wykorzystanych wcześniej maszyn wirtualnych, co powoduje częstsze zimne starty.
Wykazano zatem, że zarówno wielkość pamięci, jak i zimne starty znacząco wpływają na ogólną wydajność funkcji AWS Lambda.

Innym aspektem optymalizacji pamięci jest odpowiednie jej wykorzystanie. 
W nowoczesnych językach programowania, takich jak Java, powszechne jest użycie różnych implementacji odśmiecania pamięci (ang. garbage collection). 
Quaresma, Fireman i Pereira \cite{9235063} przeanalizowali wpływ odśmiecania pamięci w środowisku wykonawczym Java i AWS Lambda. 
Po pierwsze, wykazali oni, że użycie odśmiecania pamięci może negatywnie wpłynąć na wydajność funkcji. 
Następnie, poprzez użycie techniki  ,,Garbage Collector Control Interceptor", złagodzili negatywny wpływ GC (ang. Garbace Collector), co przyspieszyło czas odpowiedzi o około 10\% oraz zmniejszyło koszt działania o 7\%. 

Wybór odpowiedniej wielkości pamięci funkcji jest bardzo ważnym elementem wdrożenia także ze względu na bezpośredni jej wpływ na koszty. Elgamal i inni autorzy \cite{8567674} zaproponowali model optymalizacji kosztów funkcji, w którym jednym z czynników była pamięć AWS Lambda. 
Zwrócili uwagę na to, że nawet niewielkie wartości (z 128 MB do 256 MB) było wstanie poprawić szybkość wykonania algorytmu o 10\%, przy jednoczesnym obniżeniu kosztów o 6\%.

Pawlik, Figiela i Malawski \cite{pawlik2019performanceconsiderationsexecutionlarge} zwrócili uwagę na wpływ pamięci FaaS dla konkretnych zastosowań naukowych. 
Dokonali równoczesnej ewaluacji 5120 zadań z użyciem serwisów różnych dostawców (m. in. AWS). 
Duża liczba zadań wynikała z chęci przetestowania przekroczenia limitów współbieżności oferowanych przez dostawców. 
Wykazali, że wraz z wzrostem pamięci rośnie wydajność funkcji, mierzona za pomocą wskaźnika GFlops (ang. Giga Floating Point Operations Per Second). 
Interesującym szczegółem jest niewielka różnica wydajności pomiędzy 2048 MB i 3008 MB (około 0.8\%). 
Autorzy wskazują, że może to być spowodowane taką samą konfiguracją limitów procesora, gdyż wielkość 2048 do niedawna była największą oferowaną przez AWS.

\subsubsection*{Architektura procesora}

Amazon Web Services oferuje możliwość wyboru architektury procesora spośród X86\_64 oraz ARM64. Architektura ARM64 jest wspierana poprzez procesory AWS Graviton2 rozwijane przez AWS.

Chen, Hung, Cordingly oraz Lloyd \cite{10.1145/3631295.3631394} zwrócili uwagę na znaczące różnice wydajności między obiema dostępnymi architekturami procesora. Autorzy przeprowadzili testy wydajnościowe 18 funkcji AWS Lambda i działających na obu rodzajach procesorów (Intel Xeon dla X86\_64 oraz AWS Graviton2 dla ARM64). 

Wykazali oni podobne zużycie procesora dla obu architektur. Wiele funkcji wykorzystywało wyłącznie jeden z dostępnych rdzeni procesora, co wskazuje na możliwość optymalizacji w kierunku zrównoleglania obliczeń. 
Mimo podobnego zużycia, sam czas działania funkcji był zróżnicowany. 
7 z 18 funkcji działało szybciej na ARM64 (4 były ponad 10\% szybsze), podczas gdy 6 działało znacznie wolniej (o ponad 10\%). 
Funkcje ARM64 były bardziej opłacalne dla większości przypadków. 15 z 18 funkcji miało niższe szacunkowe koszty działania na ARM64 w porównaniu do X86 (jednak znaczący wpływ na to miała zniżka oferowana przez dostawcę chmurowego).

Lambion i inni autorzy \cite{10.1145/3491204.3543506} przeprowadzili analizę użycia algorytmów przetwarzania języka naturalnego z użyciem obu architektur procesora. W ramach badania przygotowali oni składający się z kilku etapów pipeline, który używał wspomniancyh algorytmów. Funkcje zostały wdrożone z użyciem obu architektur oraz w różnych regionach AWS.

Wydajność obu architektur różniła się w zależności od etapu pipeline’u. Dla regionu us-east-2 ARM64 był szybszy w przypadku funkcji przetwarzania wstępnego (o 7,3\%) i zapytań (o 8,9\%), podczas gdy X86\_64 był znacznie szybszy (o 23,6\%) w przypadku funkcji treningowej. W ujęciu globalnym funkcje ARM64 były średnio o 1.7\% szybsze niż funkcje X86\_64.

Działanie funkcji różniło się także w zależności od regionu. Funkcje w architekturze X86\_64 działały najszybciej w regionie eu-central-1, a najwolniej w us-west-2. W przypadku ARM64, region us-west-2 był najszybszy, a us-east-2 najwolniejszy. Funkcje wykazały także tendencję do bycia szybszymi poza typowymi godzinami pracy (np. funkcje w godzinach 6:00-8:00 działały o 6\% szybciej niż funkcje w godzinach 10:00-12:00).

\subsubsection*{Zimne starty}

Specyficznym zjawiskiem dla usług FaaS są tzw. zimne starty. 
Polegają one na dłuższym czasie inicjalizacji funkcji, co wynika z konieczności przygotowania infrastruktury w postaci maszyny wirtualnej i środowiska wykonawczego. 
Jest to ważny czynnik wpływający na serwisy jak AWS Lambda, w szczególności w przypadku aplikacji skierowanych do użytkowników.

Zimne starty występują często na platformie AWS, co stwierdzili Kelly, Glavin i Barrett \cite{9284261} podczas analizy infrastruktury obsługującej AWS Lambda. 
Podczas badań z użyciem powtarzających się co godzinę wywołań doświadczyli oni bardzo częstych zimnych startów funkcji (aż około 89\% uruchomień). 
Podkreśla to wielkość problemu zimnych startów w przypadku rzadko używanych funkcji. 
Zimne start na platformie AWS były jednak znacząco krótsze niż w usługach innych dostawców. 
Dla funkcji o pamięci 128 MB było to maksymalnie około 350 milisekund. 
W przypadku funkcji o większym rozmiarze pamięci opóźnienia były zbliżone. 
Na bazie porówniania różnych dostawców chmurowych autorzy podkreślili, że infrastruktura AWS Lambda ma tendencję do utrzymywania gotowych maszyn wirtualnych krócej niż inni dostawcy, co prowadzi do częstszych zimnych startów, jednak z mniejszymi opóźnieniami.

Manner i inni autorzy \cite{8605777} skupili się na występowaniu zimnych startów i wpływu różnych czynników na nie. 
Wykazali oni różnice w czasie wykonania funkcji w przypadku zimnych i ciepłych startów. 
Udowodnili, że bezpośredni wpływ na nie ma wybrany język programowania, wielkość pamięci i rodzaj wdrożenia artefaktu (ZIP lub Docker). 
Pokazuje to skomplikowanie pojęcia jakim są zimne starty.

Na istotę zimnych startów wskazali także Ebrahimi, Ghoabaei-Arani i Saboohi \cite{EBRAHIMI2024103115}, poprzez dokonanie przeglądu literatury w zakresie metod ich optymalizacji. 
Jedną z najczęściej omawianych platform w literaturze jest właśnie AWS Lambda. 
Zimne starty są mierzone poprzez m. in. opóźnienie, liczbę wystąpień, użycie pamięci i całkowity czas odpowiedzi funkcji. 
Użycie odpowiednich metryk pozwala następnie na ocenę jakości konkretnych metod optymalizacji zimnych startów.

Na popularność tematu zimnych startów wśród społeczności wskazują także Nazari i inni \cite{9732138}.
Podkreślają oni wpływ tego problemu na wydajność, gdzie często czas inicjalizacji może przewyższać czas potrzebny na wykonanie logiki biznesowej.
W ramach przeglądu literatury zauważają, że poprawa zimnych startów to jeden z kierunków rozwoju platform bezserwerowych.

\subsubsection*{Język programowania}

Znaczące różnice w wydajności pomiędzy językami w AWS Lambda podkreślili Jackson i Clynch  \cite{8605773}. Przeanalizowali oni ich wpływ na zarówno ciepłe i zimne starty oraz koszty funkcji. Testowane funkcje zostały napisane w językach .NET 2, JavaScript (dla NodeJS), Java 8, Go oraz Python 3. 

W przypadku ciepłych startów najszybszymi testowanymi językami były Python (średnio 6,13 ms) oraz  .NET 2 (średnio 6,32 ms). 
Java uplasowała się na trzecim miejscu (średnio 11,33 ms), a najgorszy okazał się Go (średnio 19,21 ms). 
Wzorce wydajności zmieniają się jednak diametralnie w przypadku zimnych startów: najszybszy dalej jest Python (średnio 2,94 ms). 
Java wykazała znacząco większe opóźnienia (391,91 ms) w porównaniu z ciepłym startem (ponad 3-krotny wzrost). 
Interesującym faktem jest około 40-krotny wzrost opóźnienia funkcji .NET 2 dla zimnych startów (średnio 2.5 sekundy). 
Czas wykonania wpłynął bezpośrednio na koszty, które różniły się nawet 13-krotnie. 
Badania nie zostały niestety wykonane na różnych wielkościach pamięci, która jest jednym z głównych składowych kosztu działania.

Cordingly i inni autorzy \cite{Cordingly2020704} zwracają uwagę na potrzebę odpowiedniego doboru języka programowania do konkretnej funkcji AWS Lambda. 
Poprzez przygotowanie procesu Transform-Load-Query, składającego się z kilku komponentów, byli w stanie dokładnie przeanalizować wpływ języków na poszczególne jego etapy. 
Badane języki mogą być podzielone na dwie grupy: kompilowane (Java, Go) oraz interpretowane (Python, Javascript). Wykazano, że żaden z języków nie był najlepszego dla każdego etapu procesu. 
Poprzez przygotowanie hybrydowego pipeline’u możliwe było osiągnięcie znaczącej poprawy opóźnień (17\%-129\% szybciej). Wybrany język ma także wpływ na czas inicjalizacji funkcji. 
W przypadku Javy wymagana jest inicjalizacja JVM, jednak nie powodowało to znacząco dłuższych zimnych startów w porównaniu z Python i Node.js. Go prezentowało jednak około 20\% dłuższe zimne starty.

Podobne porównanie zostało wykonane przez Shrestha \cite{shrestha2019lambda}, który przeanalizował natywnie wspierane języki w AWS Lambda. 
Języki interpretowane (Javascript, Python, Ruby) cechowały się szybszymi zimnymi startami niż kompilowane (Java, C\#, Go), choć w przypadku ciepłych startów Java oferowała wysoką wydajność. 
Shrestha zwraca jednak uwagę, że istnieje wiele innych czynników wpływających na wydajność. 
Podkreśla on, że w wyborze języka programowania ważnym elementem powinny być także osobiste preferencje programisty co do niego, a nie tylko wydajność.

\subsection{Metody optymalizacji funkcji w ekosystemie Java}\label{chapter:przeglad_literatury_wyniki_metody}

Drugim pytaniem badawczym rozpatrzonym w przeglądzie literatury jest ,,Jakie są istniejące metody optymalizacji wydajności funkcji AWS Lambda działających w ekosystemie Java?''.
W podrozdziale skupiono się na analizie aktualnego poziomu wiedzy na temat metod optymalizacji wydajności funkcji AWS Lambda w kontekście środowiska Java.
Pozwoli to na identyfikację obszarów, które nie zostały jeszcze dokładnie zbadane i wymagają dalszych badań.

\subsubsection*{Redukcja rozmiaru artefaktu}\label{chapter:przeglad_literatury_wyniki_redukcja_rozmiaru}

Podczas rozwoju aplikacji w ekosystemie Java ważnym etapem jest odpowiednie utworzenie artefaktu, który zawiera wszystkie zależności potrzebne do uruchomienia.
Wynikiem tego są pliki JAR, które podczas użycia w AWS Lambda muszą zostać pobrane do używanej maszyny wirtualnej, co wpływa na czas inicjalizacji.

Puripunpinyo i Samadzadeh \cite{8116416} zwrócili uwagę, że klasyczne narzędzia budowy artefaków tworzą często artefakty o dużym rozmiarze, gdzie część kodu jest niepotrzebna.
Zademonstrowali, że optymalizacja tych artefaktów pozwoli na poprawę wydajności, w tym zmniejszenie efektu zimnych startów. 
Dodatkowo, problemem może być przekroczenie limitu wielkości artefaktu dla AWS Lambda, który wynosi 50 MB.

Autorzy zaproponowali kilka technik optymalizacji, jak odpowiedni wybór wersji danej zależności, czy użycie zewnętrznego oprogramowania (jak ProGuard), co pozwoliło na redukcję rozmiaru.
Wśród zaproponowanych metod bardzą ważną dla kontekstu FaaS jest odpowiednie grupowanie artekfatów i funkcji. W pracy przedyskutowano głównie dwa podejścia: grupowanie ze względu na serwis oraz na rozmiar.

Grupowanie ze względu na serwis wynikało z chęci zmniejszenia rozmiaru artefaktu, który jak wykazali autorzy wpływa na czas zimnych startów. 
Odpowiedni podział artefaktów może pozwolić na otrzymanie rozmiaru, który pozwoli na szybsze wykonanie funkcji. Może to jednak powodować konieczność wysłania żądania do innej funkcji, w której znajduje się potrzebny kod.
Grupowanie ze względu na serwis pozwala zastąpić zapytania między funkcjami zapytaniami natywnymi w obrębie jednej funkcji, które z natury są szybsze. Strategia ta jednak prowadzi do większych artefaktów.

Metodą na zmiejszenie rozmiaru może być także użycie mniejszej liczby bibliotek. Problemem dla funkcji AWS Lambda może być za duża liczba zależności, co podkreślili Nupponen i Taibi \cite{9095731}.
Z przedstawionych przez nich problemów, które dotyczą funkcji AWS Lambda wynika, że odpowiednia budowa artefaktu może być kluczowa dla wydajności funkcji. 
Zbyt duży ich podział prowadzi do zapytań między funkcjami, podczas gdy zapytania te mogą być wolne oraz trudne do debuggowania. 
Jednocześnie, zbyt duży ilość kodu współdzielonego między funkcjami może prowadzić do zwiększenia rozmiaru funkcji i opóźnień, zatem zalecane jest stosowanie się do zasady pojedynczej odpowiedzialności funkcji. 

Problem optymalizacji artefaktów dla Javy w kontekście AWS Lambda podjęli również Chatley i Allerton w ramach prac nad frameworkiem Nimbus \cite{10.1145/3377812.3382135}.
Podkreślili, że klasyczne narzędzia (jak Maven Shade) łączą wszystkie zależności w jeden artefakt, nawet jeśli funkcja wykorzystuje niewielką ich część.
Aby temu zaradzić ich framework Nimbus wprowadza mechanizm budowy artefaktów, które zawierają wyłącznie te klasy, które są potrzebne do uruchomienia funkcji.
Autorzy wykazali, że takie podejście pozwala zmniejszyć rozmiar plików JAR, co przyczynia się do redukcji czasu zimnych startów. 
Nimbus potrafi także wykryć i wdrożyć tylko zmienione funkcje, co dodatkowo ogranicza liczbę niepotrzebnych zimnych startów.

\subsubsection*{Wybór rodzaju wdrożenia}

AWS oferuje dwie strategie wdrożenia funkcji Lambda, opierające się o pliki ZIP lub obrazy Docker. 
Świadomy ich wybór ma znaczący wpływ na wydajność, szczególnie na czas zimnych startów.
Dantas, Khazaei i Litoiu \cite{9860368} przeprowadzili szczegółowe badania porównujące obie opcje dla wybranych języków programowania.

Wyniki różniły się w zależności od wybranego języka. Dla Pythona oraz Javascriptu obie opcje działały podobnie, lub z korzyścią dla obrazów Dockerowych. 
Co ciekawe, dla języka Java tendencja była odwrotna - wdrożenie oparte o pliki ZIP zapewniało krótszy czas zimnego startu w porównaniu do wdrożenia kontenerowego. 
Wyniki te były spójne niezależnie od testowanego rozmiaru aplikacji czy ilości przydzielonej pamięci, a przewaga wdrożeń ZIP była szczególnie widoczna dla większych aplikacji.

\subsubsection*{Pingowanie}

Po wykonaniu zapytania do funkcji AWS Lambda maszyny wirtualne, które zostały wykorzystane, pozostają aktywne w oczekiwaniu na kolejne uruchomienia. 
Trwa to przeważnie kilka minut \cite{9284261}, gdy uruchomiony już kod dalej jest gotowy do działania.
Taktyką z tym związaną jest regularne uruchamianie funkcji zaproponowane przez \cite{8605779} Lloyd i innych autorów. 
Funkcje AWS Lambda były uruchamiane przez specjalne instancje EC2 lub uslugę CloudWatch, co pozwoliło utrzymać funkcje aktywne nawet do 24 godzin.
Podejście to pozwoliło na redukcję zimnych startów i przyspieszenie funkcji około czterokrotnie.
W porównaniu z klasyczną infrastrukturą (opartą o np. kontenery Docker) funkcje Lambda były około 10\% wolniejsze, jednak użycie funkcji bezserwerowych pozwoliło na około 18-krotne zmniejszenie kosztów. 

\subsubsection*{Użycie Javascript}
Kaplunovich \cite{8844428} proponuje interesujące podejście do migracji monolitycznych aplikacji napisanych w technologii Java do AWS Lambda.
Migracja ta prowadzona jest z użyciem narzędzia ToLambda, które jest w stanie transformować kod Java na JavaScript, który jest następnie uruchamiany w AWS Lambda jako funkcje NodeJS.
Autor motywuje wybór JavaScriptu jako docelowego języka ze względu na trendy wskazujące lepszą wydajność i popularność tego języka.
Dodatkowo, podkreśla takie zalety jak mniejsza szczegółowość kodu w porównaniu do Javy, brak konieczności kompilacji, co przyspiesza wdrożenia oraz dobra integracja z usługami AWS.

Z użyciem zaproponowanego narzędzia ToLambda użytkownik jest w stanie przekształcić wybraną funkcję publiczną Javy w niezależne funkcje Lambda.
Dla każdej funkcji transformowane są także wszystkie wymagane do uruchomienia zależności (jak klasy), wraz z zachowaniem ich właściowości (np. poziom dostępu do pól).
Autor zwraca jednak uwagę na skomplikowanie Javy i jej specyficznych konstrukcji (jak polimorfizm czy hierarchia konstruktorów), które stanowią wyzwanie w trakcie transformacji. 

Choć głównym celem pracy jest automatyzacja migracji monolitycznych aplikacji do architektury bezserwerowej, autor prezentuje interesujące podejście do użycia Java w AWS Lambda.
W celu uniknięcia wyzwań jak zimne starty czy czasochłonność budowy aplikacji, pośrednie użycie innego języka może być skuteczną metodą optymalizacji.

Podobny trend został także zaprezentowany w pracy Dos Santosa i innych autorów \cite{FerreiraDosSantos2023}.
Jako rozwiązanie problemu zimnych startów funkcji AWS Lambda o niskiej częstotliwości użycia, autorzy zaproponowali użycie NodeJS jako alternatywy dla Javy.
W ramach badania wykazano, że Node.js oferuje znaczącą redukcję czasu zimnego startu w porównaniu do Javy (nawet o 82\%). 
Zaobserwowano, że funkcje z pamięcią 512 MB stawały się nieaktywne już po 6-7 minutach, co czyni problem zimnego startu szczególnie istotnym dla rzadziej używanych aplikacji.

Autorzy zasugerowali zastosowanie NodeJS jako łatwiejszą w implementacji alternatywę dla bardziej złożonych technik optymalizacji Javy w środowisku AWS Lambda. 
Choć Java oferuje lepszą wydajność przy ciepłych startach, w przypadku systemów, w których wywołania funkcji następują w odstępach kilku minut, NodeJS może skutecznie ograniczyć opóźnienia związane ze startem funkcji.

Mimo interesującego podejścia, użycie JavaScript w miejsce Javy może być jednak nieoptymalne dla zespołów programistycznych z umiejętnościami w ekosystemie Java. 
Wymaga to kompletnej zmiany używanych narzędzi, co może być kosztowne. Dlatego metody te zostały zawarte w wynikach przeglądu jako prezentacja alternatywy, jednak nie jako pełne rozwiązanie problemu wydajności.

\subsubsection*{JIT}

Jedną z metod optymalizacji nowoczesnych języków programowania jest JIT (ang. just-in-time compilation), czyli metoda kompilacji fragmentów kodu do kodu maszynowego, bezpośrednio przed ich wykonaniem.
Metoda ta jest często wykorzystywana przez maszyny wirtualne Javy. Jednak według Carreira i innych autorów \cite{10.1145/3458336.3465305}, technika ta jest niewystarczająco wspierana w funkcjach AWS Lambda.
W ramach pracy zademonstrowano problem ciepłych startów, które uruchamiają niezoptymalizowany kod, co prowadzi do pogorszenia wydajności.

Autorzy zaproponowali platformę IGNITE, która wprowadza pojęcie gorących startów. Są to uruchomienia funkcji z użyciem dodatkowo zoptymalizowanego kodu (poprzez JIT).
W ramach badania, stworzona została alternatywna platforma funkcji, oparta o kontenery Docker. 
Gorące starty były możliwe poprzez ponowne użycie, już uruchomionych wcześniej kontenerów, zawierających zoptymalizowany kod.

Zaproponowana metoda prowadziła do znaczącej redukcji czasu wykonania funkcji (nawet 55-krotnego dla Javy). 
Dodatkowo, widoczny był trend coraz lepszej poprawy wydajności, wraz z kolejnymi uruchomieniami funkcji.

\subsubsection*{GraalVM}

Innym podejściem do optymalizacji funkcji Java w AWS Lambda jest wykorzystanie GraalVM.
GraalVM to wysokowydajny zestaw JDK (ang. Java Development Kit), który może przyspieszyć działanie aplikacji opartych na technologii Java.
Umożliwia kompilację AOT (ang. ahead-of-time) kodu Java do natywnego obrazu, który następnie uruchamiany jest niemal natychmiast oraz zużywa mało zasobów pamięci.

GraalVM to jedna z technik zaproponowanych przez Menendez i Bartlett \cite{menéndez2023performancebestpracticesusing}.
Autorzy wykazali, że użycie tej metody pozwoliło na przyspieszenie zimnych startów o 83\% oraz opóźnienia dla rozgrzanych funkcji o 55\%.
Zaznaczyli oni jednak, że użycie GraalVM może być bardziej skomplikowane w porównaniu do klasycznej maszyny wirtualnej Javy. 
Wynika to z konieczności użycia niestandardowych środowisk uruchomieniowych (ang. custom runtimes) w ramach AWS Lambda, które muszą zostać skonfigurowane przez programistę.

Optymalizacji z użyciem GraalVM oraz różnych frameworków Javy dokonał także Ritzal \cite{ritzal2020optimizing}.
W ramach pracy autor przeanalizował użycie frameworków SpringBoot, Micronaut oraz Quarkus, działających zarówno w ramach klasycznego JVM oraz GraalVM.
Dodatkowo, przetestowana została funkcja bazowa bez użycia żadnego z frameworków.

Wyniki eksperymentu pokazały, że w przypadku użycia zwykłego JVM funkcja bazowa charakteryzowała się najszybszymi zimnymi startami.
Funkcje oparte o frameworki były znacząco wolniejsze. Jednak podczas użycia GraalVM, funkcja oparta o Micronaut posiadała najniższe opóźnienia podczas zimnych startów.
Jednocześnie, charakteryzowała się ona niskim zużyciem pamięci.

\subsubsection*{SnapStart}

SnapStart to funkcjonalność oferowana przez AWS w celu łagodzenia problemu zimnych startów dla funkcji napisanych w Javie. 
Jest to jedna z technik zaproponowanych przez Menendeza i Bartletta w celu optymalizacji funkcji AWS Lambda opartych o Javę \cite{menéndez2023performancebestpracticesusing}. 
Autorzy wykazali, że włączenie tej opcji w testowanym systemie pozwoliło na przyspieszenie zimnych startów o 16\% oraz zmniejszenie opóźnień dla rozgrzanych funkcji o 21\%. 
Zaznaczyli oni jednak, że SnapStart posiada istotne ograniczenia, takie jak brak wsparcia dla architektury ARM64, niestandardowych środowisk uruchomieniowych (ang. custom runtimes), integracji z Amazon EFS czy możliwości połączenia funkcji z VPC.

\subsection{Cechy rozwoju aplikacji w AWS Lambda}\label{chapter:przeglad_literatury_wyniki_cechy_rozwoju}

W podrozdziale rozpatrzono prace poruszające trzecie pytanie badawcze ,,Jakie są cechy rozwoju aplikacji w architekturze bezserwerowej AWS Lambda?''.
Skupiono się aspektach, które muszą zostać poruszone przez programistę podczas tworzenia systemów w architekturze bezserwerowej.
Identyfikacja tych cech pozwoli później na dokładniejszą analizę wpływu konkretnych metod optymalizacji wydajności na te cechy.

\subsubsection*{Zarządzanie wielkością funkcji}

Zbyte duże rozdrobnienie funkcji może powodować wiele pośrednich problemów, które zostały przedstawione przez Nupponen i Taibi \cite{9095731}.
Na bazie doświadczeń programistów, zebrali oni najczęstsze problemy związane z rozwojem oprogramowania w aplikacjach bezserwerowych.
Bezpośrednio wskazują oni na problem zbyt wielu funkcji, który może utrudnić utrzymanie i zrozumienie systemu.
Problemem jest także komunikacja między funkcjami, która może być szczególnie wymagana przy dużej ich liczbie. 
Autorzy podkreślają, że ,,asynchroniczne wywołania do i pomiędzy funkcjami bezserwerowymi zwiększają złożoność systemu''\cite{9095731}, 
a problemem bezpośrednich wywołań jest ,,Złożone debugowanie, luźna izolacja funkcji. Dodatkowe koszty, jeśli funkcje są wywoływane synchronicznie, ponieważ musimy płacić za dwie funkcje działające w tym samym czasie.''\cite{9095731}
W przypadku zbyt dużych funkcji, problematyczne staje się także użycie zbyt dużej liczby bibliotek, co zwiększa ryzyko przekroczenia limitu wielkości artefaktu funkcji.

W kolejnej pracy Taibi kontynuuje analizy w tym zakresie wraz z Kehoe i Poccia \cite{9912641}. 
Wykonali oni badanie ankietowe wśród doświadczonych praktyków pracujących z aplikacjami w architekturach bezserwerowych.
Aż 38.46\% badanych wskazało, że synchroniczne zapytania między funkcjami mają znaczny negatywny wpływ na stan aplikacji.
Także około co 5. badany wskazał, że złą praktyką jest także dzielenie tego samego kodu między wieloma funkcjami.
Same funkcje powinny być skupione wyłącznie na pojedynczym zadaniu biznesowym.

Jak wykazali Eismann i inni \cite{eismann2021reviewserverlessusecases} systemy składające się powyżej 5 funkcji są bardzo rzadkie.
W ich badaniach aż 82\% analizowanych przypadków użycia zawierało pięć funkcji lub mniej, a 93\% mniej niż dziesięć. 
Według autorów wynika to potencjalnie z dwóch głównych czynników:
\begin{quote}
    ,,Po pierwsze, bezserwerowe modele aplikacji zmniejszają ilość kodu, który programiści muszą napisać, ponieważ pozwalają im skupić się na logice biznesowej (\dots). Po drugie, wydaje się to wskazywać, że programiści wybierają obecnie raczej dużą ziarnistość dla rozmiaru funkcji bezserwerowych'' \cite{eismann2021reviewserverlessusecases}
\end{quote}
Eismann i inni autorzy podkreślają, że optymalizacja wielkości funkcji jest interesującym tematem do dalszych badań.

W badaniach Leitnera i innych \cite{LEITNER2019340} potwierdzono, że aplikacje bezserwerowe zazwyczaj składają się z niewielkiej liczby funkcji.
Aż 64\% badanych wskazało, że ich aplikacje zawierają od 1 do 10 funkcji, co jest zbliżone do wyników Eismanna.
W kontekście granularności funkcji, najczęściej stosowaną praktyką była drobna granularność, z funkcjami przypisanymi do pojedynczych metod REST (36\% badanych).
Jednak większa liczba funkcji może prowadzić do problemów z zarządzaniem, testowaniem i brakiem odpowiednich narzędzi do monitorowania, co stanowi wyzwanie w praktyce.

\subsubsection*{Integracja z zewnętrznymi usługami}

Rozwój aplikacji w modelu serverless opiera się w dużej mierze na integracji z innymi usługami.
Wynika to znacznie z bezstanowości AWS Lambda, które uruchamiane są na żądanie.
Konieczność integracji podkreślają autorzy Ivanon i Petrova \cite{Ivanov_Petrova_2024}.
Pozwala to budować kompletne systemy w oparciu o funkcje AWS Lambda oraz zarządzane serwisy chmurowe.
Kluczową rolę odgrywają tutaj różnego rodzaju wyzwalacze zdarzeń (na przykład zmiany danych w Amazon S3 czy DynamoDB) i wiadomości przesyłane przez Amazon SNS.
AWS Lambda integruje się także z usługami takimi jak API Gateway, EventBridge czy Step Functions.
Pozwala to budować pełne aplikacje bez własnej infrastruktury.
Autorzy wskazują, że rozwój aplikacji serverless opiera się na łączeniu funkcji z zarządzanymi usługami.

Bezstanowość funkcji AWS Lambda wymusza integrację z zewnętrznymi serwisami, takimi jak bazy danych czy systemy plików.
Ghosh i inni \cite{9027427} zwracają uwagę, że taka komunikacja odbywa się po sieci i może znacząco zwiększać opóźnienia działania aplikacji.
W ich badaniach dostęp do Amazon DynamoDB z funkcji Lambda był prawie czternastokrotnie wolniejszy niż dostęp do lokalnej bazy danych z tradycyjnej aplikacji.
Problem ten narasta w bardziej złożonych systemach, gdzie sieciowe opóźnienia sumują się na ścieżce krytycznej.
Autorzy podkreślają, że problem wynika z samej architektury serverless, gdzie funkcje są uruchamiane w izolacji.
Mimo wad związanych z integracjami z serwisami zewnętrznymi, pozostają one konieczne w rozwoju systemów bezserwerowych.

Na powszechność integracji z serwisami chmurowymi wskazali również Eismann i inni \cite{eismann2021reviewserverlessusecases}.
Dokonali oni analizy przypadków użycia serverless z projektów otwartoźródłowych, białej i szarej literatury oraz konsultacji z ekspertammi.
Wykazali oni, że aplikacje najczęściej korzystały z zewnętrznych rozwiązań przechowywania danych (np. Amazon S3) i baz danych (np. DynamoDB).
Było to odpowiednio 61\% i 47\% analizowanych systemów.
Użycie takich usług było spodziewane ze względu na bezstanowy charakter usług FaaS.
Interesującym wynikiem badania jest, że jedynie 18\% funkcji korzysta z rozwiązań API Gateway (bramka API).

\subsubsection*{Odpowiedni język programowania}

Usługa AWS Lambda oferuje wsparcie dla różnych języków programowania. 
Ich odpowiedni wybór może być kluczowy ze względu na np. wydajność.
Kluczową rolę tej decyzji w cyklu rozwoju oprogramowania wskazali Raza i inni autorzy \cite{raza2021sok}.
Skupili się oni na modelu FaaS z perspektywy programisty, rozwijającego systemu w oparciu o te usługi.
Decyzja o wyborze języka została uznana przez nich jako jednorazowa.
Jej wybór może być podyktowany na przykład kosztem funkcji czy wymaganą wydajnością.
Zaznaczają oni jednak, że zmiana ,,wiązałaby się ze znacznymi kosztami rozwoju i wdrożenia, dlatego deweloper może podjąć taką decyzję tylko raz w cyklu życia aplikacji'' \cite{raza2021sok}.
Zmiana innych parametrów (jak np. wielkości pamięci) ma mniejsze konsekwencje i według programista dokonuje ich bez większego wysiłku.
Mogą one jednak wymagać utworzenia odpowiedniego sposobu ich kontroli i aktualizacji, opartego o analizę ich wpływu na wydajność.

Bardsley, Ryan i Howard \cite{8513710} podkreślają, że wybór języka programowania wpływa bezpośrednio na późniejszy rozwój aplikacji. 
Odpowiednie decyzje mogą ograniczyć opóźnienia i poprawić wydajność systemu. 
Autorzy wskazują, że różne komponenty aplikacji mogą używać różnych języków, w zależności od charakterystyki wywołań funkcji i wymagań wydajnościowcyh. 
W funkcjach reagujących na działania użytkownika (na przykład wykonanie akcji na stronie, jak kliknięcie przycisku) lepiej sprawdzają się języki interpretowane.
Mogą to być na przykład Python czy Node.js, które szybciej inicjalizują kontener. 
Funkcje przetwarzające duże ilości danych lub działające w tle mogą natomiast korzystać z języków kompilowanych, takich jak Java czy C\#.
W takim podejściu każdy element systemu optymalizowany jest indywidualnie. 
Pozwala to zmniejszyć czas odpowiedzi tam, gdzie jest to kluczowe oraz zwiększyć wydajność tam, gdzie operacje są bardziej kosztowne.

\subsubsection*{Testowanie}

Narzędziami, które mogą znacząco poprawić doświadczenie programistów pracujących z technologiami bezserwerowymi są różnego rodzaju frameworki. 
Ich analizy dokonali Skrzypek i Kritikos \cite{8605774}, którzy w swoim przeglądzie wskazali na kluczowe cechy tych narzędzi wspierające zespoły programistyczne.
Zwrócili oni uwagę na wsparcie dla testowania, poprzez lokalne lub zdalne wywołanie wykonanie funkcji.
Tylko niektóre narzędzia w ograniczony sposób wspierały testy jednostkowe (np. framework ,,Fn'' dla Javy i JavaScript oraz framework ,,Serverless'' dla Javascript).
W narzędziach brakuje jednak wsparcia dla testów integracyjnych.

Chatley i Allerton \cite{10.1145/3377812.3382135} w swojej pracy nad frameworkiem Nimbus również mocno podkreślają, że testowanie jest jednym z głównych wyzwań w trakcie tworzenia aplikacji bezserwerowych.
Autorzy na bazie badań ankietowych i własnych doświadczeń projektowych stwierdzają, że aktualne metody testowania są często trudne, powolne i kosztowne.
Framework Nimbus wprowadza możliwość uruchomienia kompletnej aplikacji w lokalnym środowisku, które symuluje docelową infrastrukturę chmurową.
Pozwala to na wykonanie testów integracyjnych dla funkcji w języku Java.

Testowanie było także cechą podejścia bezserwerowego, wskazanego przez Cavalheiro i Schepke \cite{Cavalheiro202389}.
Dokonali oni implementacji aplikacji z użyciem narzędzi AWS Lambda, Chalice (framework oparty o AWS Lambda) oraz biblioteki Flask (działającej lokalnie, jako podejście tradycyjne).
Zwrócili oni uwagę na o wiele łatwiejsze testowanie z użyciem Flask. 
Umożliwiał on rozwój testów jednostkowych API, co było utrudnione w przypadku modelu serverless.
W przypadku rozwoju AWS Lambda i Chalice trudności sprawiał także proces debuggowania.

Znaczenie testów jednostkowych podkreśla także Leitner i inni autorzy \cite{LEITNER2019340}.
Poprzez wykonane badania ankietowe aż 87\% badanych programistów wykorzystuje ten rodzaj testów w trakcie rozwoju oprogramowania serverless.
Także 55\% uważa, że aktualnie dostępne narzędzia są niewystarczające w obszarach jak testowanie czy wdrożenie.

\subsection{Podsumowanie wyników przeglądu}\label{chapter:przeglad_literatury_wyniki_podsumowanie}

TODO: napisac wnioski
