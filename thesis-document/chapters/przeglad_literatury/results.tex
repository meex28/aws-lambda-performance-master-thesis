\section{Wyniki przeglądu}\label{chapter:przeglad_literatury_wyniki}

W ramach przeglądu wybrano 33 prace badawcze, które zostały przedstawione w Tabeli \ref{table:research_papers_results}. 
Na bazie prac rozpatrzono postawione pytania badawcze. 
Odpowiedzi na nie zostały zawarte w kolejnych podrozdziałach.

\input{chapters/przeglad_literatury/results_papers.tex}

\subsection{Czynniki wpływające na wydajność funkcji}\label{chapter:przeglad_literatury_wyniki_czynniki}

Pierwszym pytaniem badawczym postawionym do przeglądu literatury jest: ,,Jakie są główne czynniki wpływające na wydajność funkcji AWS Lambda?''.
Identyfikacja czynników wpływających na wydajność jest kluczowa w kontekście jej optymalizacji.
Pozwoli to następnie na zrozumienie na które z czynników ma także wpływ twórca funkcji AWS Lambda, co ułatwi dalszą analizę metod poprawy ich wydajności.

\subsubsection*{Wielkosć pamięci funkcji}

Kelly, Glavin i Barrett \cite{9284261} zauważają, że wielkość pamięci funkcji oprócz bezpośredniego wpływu na całkowity czas działania, wywiera także wpływ na inne czynniki jak użycie procesora czy wydajność I/O dysku. 
W ramach badania wykonano pomiary dla funkcji bezserwerowych oferowanych przez wielu dostawców chmurowych, w tym Amazon Web Services, w celu zrozumienia infrastruktury i jej zarządzania, co domyślnie jest ukryte dla użytkownika.   
Poprzez analizę maszyn wirtualnych, w ramach których uruchamiany jest kod funkcji, możliwe było otrzymanie wartości parametrów, które nie są domyślnie konfigurowalne podczas wdrażania funkcji przez programistę.

Pomiary pokrywały wiele parametrów funkcji, m. in. łączny czas wykonania, czas inicjalizacji, użycie procesora, wydajność I/O dysku oraz liczbę utworzonych maszyn wirtualnych (co wpływa na częstość zimnych startów). 
W badaniu uwzględniono predefiniowane wielkości pamięci, które mogą być wybrane przez programistę (128MB, 256MB, 512MB, 1024MB oraz 2048MB).
Wraz z wzrostem pamięci funkcji, parametry te poprawiały się.

Autorzy podkreślili ważność odpowiedniego doboru wielkości pamięci podczas tworzenia funkcji. 
Dodatkowo, wykazali, że w przypadku kolejnych wywołań funkcji, platforma AWS ogranicza ponowne użycie wykorzystanych wcześniej maszyn wirtualnych, co powoduje częstsze zimne starty.
Wykazano zatem, że zarówno wielkość pamięci, jak i zimne starty znacząco wpływają na ogólną wydajność funkcji AWS Lambda.

Innym aspektem optymalizacji pamięci jest odpowiednie jej wykorzystanie. 
W nowoczesnych językach programowania, takich jak Java, powszechne jest użycie różnych implementacji odśmiecania pamięci (ang. garbage collection). 
Quaresma, Fireman i Pereira \cite{9235063} przeanalizowali wpływ odśmiecania pamięci w środowisku wykonawczym Java i AWS Lambda. 
Po pierwsze, wykazali oni, że użycie odśmiecania pamięci może negatywnie wpłynąć na wydajność funkcji. 
Następnie, poprzez użycie techniki  ,,Garbage Collector Control Interceptor", złagodzili negatywny wpływ GC (ang. Garbace Collector), co przyspieszyło czas odpowiedzi o około 10\% oraz zmniejszyło koszt działania o 7\%. 

Wybór odpowiedniej wielkości pamięci funkcji jest bardzo ważnym elementem wdrożenia także ze względu na bezpośredni jej wpływ na koszty. Elgamal i inni autorzy \cite{8567674} zaproponowali model optymalizacji kosztów funkcji, w którym jednym z czynników była pamięć AWS Lambda. 
Zwrócili uwagę na to, że wzrost o nawet niewielkie wartości (z 128 MB do 256 MB) był w stanie poprawić szybkość wykonania algorytmu o 10\%, przy jednoczesnym obniżeniu kosztów o 6\%.

Pawlik, Figiela i Malawski \cite{pawlik2019performanceconsiderationsexecutionlarge} zwrócili uwagę na wpływ pamięci FaaS dla konkretnych zastosowań naukowych. 
Dokonali równoczesnej ewaluacji 5120 zadań z użyciem serwisów różnych dostawców (m. in. AWS). 
Duża liczba zadań wynikała z chęci przetestowania przekroczenia limitów współbieżności oferowanych przez dostawców. 
Wykazali, że wraz z wzrostem pamięci rośnie wydajność funkcji, mierzona za pomocą wskaźnika GFlops (ang. Giga Floating Point Operations Per Second). 
Interesującym szczegółem jest niewielka różnica wydajności pomiędzy 2048 MB i 3008 MB (około 0.8\%). 
Autorzy wskazują, że może to być spowodowane taką samą konfiguracją limitów procesora, gdyż wielkość 2048 do niedawna była największą oferowaną przez AWS.

Cordingly i inni \cite{9946331} zaproponowali podejście CPU Time Accounting Memory Selection (CPU-TAMS), które pozwala na optymalizację wielkości pamięci.
Bazuje ono na szczegółowych metrykach czasu procesora, takich jak czas w trybie użytkownika, jądra czy oczekiwanie na I/O.
Celem jest znalezienie wielkości pamięci, która zapewnia najlepszy stosunek wydajności do kosztu, a nie tylko najniższego kosztu lub najkrótszego czasu wykonania.
Badania wykazały, że zasoby takie jak czas procesora, przepustowość I/O oraz sieci rosną wraz z wielkością pamięci.
Jednak wzrost ten nie jest zawsze liniowy, często osiągając plateau przy wielkościach pamięci około 1.5-2GB dla I/O i sieci. 

\subsubsection*{Architektura procesora}

Amazon Web Services oferuje możliwość wyboru architektury procesora spośród X86\_64 oraz ARM64. Architektura ARM64 jest wspierana poprzez procesory AWS Graviton2 rozwijane przez AWS.

Chen, Hung, Cordingly oraz Lloyd \cite{10.1145/3631295.3631394} zwrócili uwagę na znaczące różnice wydajności między obiema dostępnymi architekturami procesora. Autorzy przeprowadzili testy wydajnościowe 18 funkcji AWS Lambda i działających na obu rodzajach procesorów (Intel Xeon dla X86\_64 oraz AWS Graviton2 dla ARM64). 

Wykazali oni podobne zużycie procesora dla obu architektur. Wiele funkcji wykorzystywało wyłącznie jeden z dostępnych rdzeni procesora, co wskazuje na możliwość optymalizacji w kierunku zrównoleglania obliczeń. 
Mimo podobnego zużycia, sam czas działania funkcji był zróżnicowany. 
7 z 18 funkcji działało szybciej na ARM64 (4 były ponad 10\% szybsze), podczas gdy 6 działało znacznie wolniej (o ponad 10\%).
Mimo to, funkcje oparte o procesory w architekturze ARM64 (procesory Graviton2) okazały się bardziej opłacalne w większości przypadków, oferując lepszy stosunek kosztu do wydajności.
15 z 18 funkcji miało niższe szacunkowe koszty działania na ARM64 w porównaniu do X86 (jednak znaczący wpływ na to miała zniżka oferowana przez dostawcę chmurowego).

Lambion i inni autorzy \cite{10.1145/3491204.3543506} przeprowadzili analizę użycia algorytmów przetwarzania języka naturalnego z użyciem obu architektur procesora. W ramach badania przygotowali oni składający się z kilku etapów pipeline, który używał wspomniancyh algorytmów. Funkcje zostały wdrożone z użyciem obu architektur oraz w różnych regionach AWS.

Wydajność obu architektur różniła się w zależności od etapu pipeline’u. Dla regionu us-east-2 ARM64 był szybszy w przypadku funkcji przetwarzania wstępnego (o 7,3\%) i zapytań (o 8,9\%), podczas gdy X86\_64 był znacznie szybszy (o 23,6\%) w przypadku funkcji treningowej. W ujęciu globalnym funkcje ARM64 były średnio o 1.7\% szybsze niż funkcje X86\_64.

Działanie funkcji różniło się także w zależności od regionu. Funkcje w architekturze X86\_64 działały najszybciej w regionie eu-central-1, a najwolniej w us-west-2. W przypadku ARM64, region us-west-2 był najszybszy, a us-east-2 najwolniejszy. Funkcje wykazały także tendencję do bycia szybszymi poza typowymi godzinami pracy (np. funkcje w godzinach 6:00-8:00 działały o 6\% szybciej niż funkcje w godzinach 10:00-12:00).

\subsubsection*{Zimne starty}

Specyficznym zjawiskiem dla usług FaaS są tzw. zimne starty. 
Polegają one na dłuższym czasie inicjalizacji funkcji, co wynika z konieczności przygotowania infrastruktury w postaci maszyny wirtualnej i środowiska wykonawczego. 
Jest to ważny czynnik wpływający na serwisy jak AWS Lambda, w szczególności w przypadku aplikacji skierowanych do użytkowników.

Zimne starty występują często na platformie AWS, co stwierdzili Kelly, Glavin i Barrett \cite{9284261} podczas analizy infrastruktury obsługującej AWS Lambda. 
Podczas badań z użyciem powtarzających się co godzinę wywołań doświadczyli oni bardzo częstych zimnych startów funkcji (aż około 89\% uruchomień). 
Podkreśla to wielkość problemu zimnych startów w przypadku rzadko używanych funkcji. 
Zimne starty na platformie AWS były jednak znacząco krótsze niż w usługach innych dostawców. 
Dla funkcji o pamięci 128 MB było to maksymalnie około 350 milisekund. 
W przypadku funkcji o większym rozmiarze pamięci opóźnienia były zbliżone. 
Na bazie porówniania różnych dostawców chmurowych autorzy podkreślili, że infrastruktura AWS Lambda ma tendencję do utrzymywania gotowych maszyn wirtualnych krócej niż inni dostawcy, co prowadzi do częstszych zimnych startów, jednak z mniejszymi opóźnieniami.

Manner i inni autorzy \cite{8605777} skupili się na występowaniu zimnych startów i wpływu różnych czynników na nie. 
Wykazali oni różnice w czasie wykonania funkcji w przypadku zimnych i ciepłych startów. 
Udowodnili, że bezpośredni wpływ na nie ma wybrany język programowania, wielkość pamięci i rodzaj wdrożenia artefaktu (ZIP lub Docker). 
Pokazuje to skomplikowanie pojęcia jakim są zimne starty.

Na istotę zimnych startów wskazali także Ebrahimi, Ghoabaei-Arani i Saboohi \cite{EBRAHIMI2024103115}, poprzez dokonanie przeglądu literatury w zakresie metod ich optymalizacji. 
Jedną z najczęściej omawianych platform w literaturze jest właśnie AWS Lambda. 
Zimne starty są mierzone poprzez m. in. opóźnienie, liczbę wystąpień, użycie pamięci i całkowity czas odpowiedzi funkcji. 
Użycie odpowiednich metryk pozwala następnie na ocenę jakości konkretnych metod optymalizacji zimnych startów.

Na popularność tematu zimnych startów wśród społeczności wskazują także Nazari i inni \cite{9732138}.
Podkreślają oni wpływ tego problemu na wydajność, gdzie często czas inicjalizacji może przewyższać czas potrzebny na wykonanie logiki biznesowej.
W ramach przeglądu literatury zauważają, że poprawa zimnych startów to jeden z kierunków rozwoju platform bezserwerowych.

AWS Lambda wyróżnia się na tle innych platform FaaS w kontekście zimnych startów.
Yu i inni autorzy \cite{10.1145/3419111.3421280} podkreślili różnicę w technologiach używanych przez dostawców funkcji bezserwerowych.
Funkcje AWS Lambda uruchamiane są z użyciem mikro maszyn wirtualnych Firecracker.
Pozwala to na osiągnięcie lepszych zimnych startów niż występujące w innych platformach (np. Google Cloud Platform).
Autorzy zauważają jednak, że zimne starty dalej znacząco wydłużają czas odpowiedzi.
Wynika to z potrzeby pobrania kodu (np. z S3) oraz inicjalizacji funkcji.

\subsubsection*{Język programowania}

Znaczące różnice w wydajności pomiędzy językami w AWS Lambda podkreślili Jackson i Clynch  \cite{8605773}. Przeanalizowali oni ich wpływ na zarówno ciepłe i zimne starty oraz koszty funkcji. Testowane funkcje zostały napisane w językach .NET 2, JavaScript (dla NodeJS), Java 8, Go oraz Python 3. 

W przypadku ciepłych startów najszybszymi testowanymi językami były Python (średnio 6,13 ms) oraz  .NET 2 (średnio 6,32 ms). 
Java uplasowała się na trzecim miejscu (średnio 11,33 ms), a najgorszy okazał się Go (średnio 19,21 ms). 
Wzorce wydajności zmieniają się jednak diametralnie w przypadku zimnych startów: najszybszy dalej jest Python (średnio 2,94 ms). 
Java wykazała znacząco większe opóźnienia (391,91 ms) w porównaniu z ciepłym startem (ponad trzykrotny wzrost). 
Interesującym faktem jest około czterdziestokrotny wzrost opóźnienia funkcji .NET 2 dla zimnych startów (średnio 2.5 sekundy). 
Czas wykonania wpłynął bezpośrednio na koszty, które różniły się nawet trzynastokrotnie. 
Badania nie zostały niestety wykonane na różnych wielkościach pamięci, która jest jedną z głównych składowych kosztu działania.

Cordingly i inni autorzy \cite{Cordingly2020704} zwracają uwagę na potrzebę odpowiedniego doboru języka programowania do konkretnej funkcji AWS Lambda. 
Poprzez przygotowanie procesu Transform-Load-Query, składającego się z kilku komponentów, byli w stanie dokładnie przeanalizować wpływ języków na poszczególne jego etapy. 
Badane języki mogą być podzielone na dwie grupy: kompilowane (Java, Go) oraz interpretowane (Python, Javascript). Wykazano, że żaden z języków nie był najlepszy dla każdego etapu procesu. 
Poprzez przygotowanie hybrydowego pipeline’u możliwe było osiągnięcie znaczącej poprawy opóźnień (17\%-129\% szybciej). Wybrany język ma także wpływ na czas inicjalizacji funkcji. 
W przypadku Javy wymagana jest inicjalizacja JVM, jednak nie powodowało to znacząco dłuższych zimnych startów w porównaniu z Python i Node.js. Go prezentowało jednak około 20\% dłuższe zimne starty.

Podobne porównanie zostało wykonane przez Shrestha \cite{shrestha2019lambda}, który przeanalizował natywnie wspierane języki w AWS Lambda. 
Języki interpretowane (Javascript, Python, Ruby) cechowały się szybszymi zimnymi startami niż kompilowane (Java, C\#, Go), choć w przypadku ciepłych startów Java oferowała wysoką wydajność. 
Shrestha zwraca jednak uwagę, że istnieje wiele innych czynników wpływających na wydajność. 
Podkreśla on, że w wyborze języka programowania ważnym elementem powinny być także osobiste preferencje programisty co do niego, a nie tylko wydajność.

\subsection{Metody optymalizacji funkcji w ekosystemie Java}\label{chapter:przeglad_literatury_wyniki_metody}

Drugim pytaniem badawczym rozpatrzonym w przeglądzie literatury jest ,,Jakie są istniejące metody optymalizacji wydajności funkcji AWS Lambda działających w ekosystemie Java?''.
W podrozdziale skupiono się na analizie aktualnego poziomu wiedzy na temat metod optymalizacji wydajności funkcji AWS Lambda w kontekście środowiska Java.
Pozwoli to na identyfikację obszarów, które nie zostały jeszcze dokładnie zbadane i wymagają dalszych badań.

\subsubsection*{Redukcja rozmiaru artefaktu}\label{chapter:przeglad_literatury_wyniki_redukcja_rozmiaru}

Podczas rozwoju aplikacji w ekosystemie Java ważnym etapem jest odpowiednie utworzenie artefaktu, który zawiera wszystkie zależności potrzebne do uruchomienia.
Wynikiem tego są pliki JAR, które podczas użycia w AWS Lambda muszą zostać pobrane do używanej maszyny wirtualnej, co wpływa na czas inicjalizacji.

Puripunpinyo i Samadzadeh \cite{8116416} zwrócili uwagę, że klasyczne narzędzia budowy artefaków tworzą często artefakty o dużym rozmiarze, gdzie część kodu jest niepotrzebna.
Zademonstrowali, że optymalizacja tych artefaktów pozwoli na poprawę wydajności, w tym zmniejszenie efektu zimnych startów. 
Dodatkowo, problemem może być przekroczenie limitu wielkości artefaktu dla AWS Lambda, który wynosi 50 MB.

Autorzy zaproponowali kilka technik optymalizacji, jak odpowiedni wybór wersji danej zależności, czy użycie zewnętrznego oprogramowania (jak ProGuard), co pozwoliło na redukcję rozmiaru.
Wśród zaproponowanych metod bardzą ważną dla kontekstu FaaS jest odpowiednie grupowanie artekfatów i funkcji. W pracy przedyskutowano głównie dwa podejścia: grupowanie ze względu na serwis oraz na rozmiar.

Grupowanie ze względu na serwis wynikało z chęci zmniejszenia rozmiaru artefaktu, który jak wykazali autorzy wpływa na czas zimnych startów. 
Odpowiedni podział artefaktów może pozwolić na otrzymanie rozmiaru, który pozwoli na szybsze wykonanie funkcji. Może to jednak powodować konieczność wysłania żądania do innej funkcji, w której znajduje się potrzebny kod.
Grupowanie ze względu na serwis pozwala zastąpić zapytania między funkcjami zapytaniami natywnymi w obrębie jednej funkcji, które z natury są szybsze. Strategia ta jednak prowadzi do większych artefaktów.

Metodą na zmiejszenie rozmiaru może być także użycie mniejszej liczby bibliotek. Problemem dla funkcji AWS Lambda może być za duża liczba zależności, co podkreślili Nupponen i Taibi \cite{9095731}.
Z przedstawionych przez nich problemów, które dotyczą funkcji AWS Lambda wynika, że odpowiednia budowa artefaktu może być kluczowa dla wydajności funkcji. 
Zbyt duży ich podział prowadzi do zapytań między funkcjami, podczas gdy zapytania te mogą być wolne oraz trudne do debuggowania. 
Jednocześnie, zbyt duży ilość kodu współdzielonego między funkcjami może prowadzić do zwiększenia rozmiaru funkcji i opóźnień, zatem zalecane jest stosowanie się do zasady pojedynczej odpowiedzialności funkcji. 

Problem optymalizacji artefaktów dla Javy w kontekście AWS Lambda podjęli również Chatley i Allerton w ramach prac nad frameworkiem Nimbus \cite{10.1145/3377812.3382135}.
Podkreślili, że klasyczne narzędzia (jak Maven Shade) łączą wszystkie zależności w jeden artefakt, nawet jeśli funkcja wykorzystuje niewielką ich część.
Aby temu zaradzić ich framework Nimbus wprowadza mechanizm budowy artefaktów, które zawierają wyłącznie te klasy, które są potrzebne do uruchomienia funkcji.
Autorzy wykazali, że takie podejście pozwala zmniejszyć rozmiar plików JAR, co przyczynia się do redukcji czasu zimnych startów. 
Nimbus potrafi także wykryć i wdrożyć tylko zmienione funkcje, co dodatkowo ogranicza liczbę niepotrzebnych zimnych startów.

\subsubsection*{Wybór rodzaju wdrożenia}

AWS oferuje dwie strategie wdrożenia funkcji Lambda, opierające się o pliki ZIP lub obrazy Docker. 
Świadomy ich wybór ma znaczący wpływ na wydajność, szczególnie na czas zimnych startów.
Dantas, Khazaei i Litoiu \cite{9860368} przeprowadzili szczegółowe badania porównujące obie opcje dla wybranych języków programowania.

Wyniki różniły się w zależności od wybranego języka. Dla Pythona oraz Javascriptu obie opcje działały podobnie, lub z korzyścią dla obrazów Dockerowych. 
Co ciekawe, dla języka Java tendencja była odwrotna - wdrożenie oparte o pliki ZIP zapewniało krótszy czas zimnego startu w porównaniu do wdrożenia kontenerowego. 
Wyniki te były spójne niezależnie od testowanego rozmiaru aplikacji czy ilości przydzielonej pamięci, a przewaga wdrożeń ZIP była szczególnie widoczna dla większych aplikacji.

\subsubsection*{Pingowanie}

Po wykonaniu zapytania do funkcji AWS Lambda maszyny wirtualne, które zostały wykorzystane, pozostają aktywne w oczekiwaniu na kolejne uruchomienia. 
Trwa to przeważnie kilka minut \cite{9284261}, gdy uruchomiony już kod dalej jest gotowy do działania.
Taktyką z tym związaną jest regularne uruchamianie funkcji zaproponowane przez Lloyd i innych autorów \cite{8605779}. 
Funkcje AWS Lambda były uruchamiane przez specjalne instancje EC2 lub uslugę CloudWatch, co pozwoliło utrzymać funkcje aktywne nawet do 24 godzin.
Podejście to pozwoliło na redukcję zimnych startów i przyspieszenie funkcji około czterokrotnie.
W porównaniu z klasyczną infrastrukturą (opartą o np. kontenery Docker) funkcje Lambda były około 10\% wolniejsze, jednak użycie funkcji bezserwerowych pozwoliło na około 18-krotne zmniejszenie kosztów. 

\subsubsection*{Użycie Javascript}
Kaplunovich \cite{8844428} proponuje interesujące podejście do migracji monolitycznych aplikacji napisanych w technologii Java do AWS Lambda.
Migracja ta prowadzona jest z użyciem narzędzia ToLambda, które jest w stanie transformować kod Java na JavaScript, który jest następnie uruchamiany w AWS Lambda jako funkcje NodeJS.
Autor motywuje wybór JavaScriptu jako docelowego języka ze względu na trendy wskazujące lepszą wydajność i popularność tego języka.
Dodatkowo, podkreśla takie zalety jak mniejsza szczegółowość kodu w porównaniu do Javy, brak konieczności kompilacji, co przyspiesza wdrożenia oraz dobra integracja z usługami AWS.

Z użyciem zaproponowanego narzędzia ToLambda użytkownik jest w stanie przekształcić wybraną funkcję publiczną Javy w niezależne funkcje Lambda.
Dla każdej funkcji transformowane są także wszystkie wymagane do uruchomienia zależności (jak klasy), wraz z zachowaniem ich właściowości (np. poziom dostępu do pól).
Autor zwraca jednak uwagę na skomplikowanie Javy i jej specyficznych konstrukcji (jak polimorfizm czy hierarchia konstruktorów), które stanowią wyzwanie w trakcie transformacji. 

Choć głównym celem pracy jest automatyzacja migracji monolitycznych aplikacji do architektury bezserwerowej, autor prezentuje interesujące podejście do użycia Java w AWS Lambda.
W celu uniknięcia wyzwań jak zimne starty czy czasochłonność budowy aplikacji, pośrednie użycie innego języka może być skuteczną metodą optymalizacji.

Podobny trend został także zaprezentowany w pracy Dos Santos i innych autorów \cite{FerreiraDosSantos2023}.
Jako rozwiązanie problemu zimnych startów funkcji AWS Lambda o niskiej częstotliwości użycia, autorzy zaproponowali użycie NodeJS jako alternatywy dla Javy.
W ramach badania wykazano, że Node.js oferuje znaczącą redukcję czasu zimnego startu w porównaniu do Javy (nawet o 82\%). 
Zaobserwowano, że funkcje z pamięcią 512 MB stawały się nieaktywne już po 6-7 minutach, co czyni problem zimnego startu szczególnie istotnym dla rzadziej używanych aplikacji.

Autorzy zasugerowali zastosowanie NodeJS jako łatwiejszą w implementacji alternatywę dla bardziej złożonych technik optymalizacji Javy w środowisku AWS Lambda. 
Choć Java oferuje lepszą wydajność przy ciepłych startach, w przypadku systemów, w których wywołania funkcji następują w odstępach kilku minut, NodeJS może skutecznie ograniczyć opóźnienia związane ze startem funkcji.

Mimo interesującego podejścia, użycie JavaScript w miejsce Javy może być jednak nieoptymalne dla zespołów programistycznych z umiejętnościami w ekosystemie Java. 
Wymaga to kompletnej zmiany używanych narzędzi, co może być kosztowne. Dlatego metody te zostały zawarte w wynikach przeglądu jako prezentacja alternatywy, jednak nie jako pełne rozwiązanie problemu wydajności.

\subsubsection*{JIT}

Jedną z metod optymalizacji nowoczesnych języków programowania jest JIT (ang. just-in-time compilation), czyli metoda kompilacji fragmentów kodu do kodu maszynowego, bezpośrednio przed ich wykonaniem.
Metoda ta jest często wykorzystywana przez maszyny wirtualne Javy. Jednak według Carreira i innych autorów \cite{10.1145/3458336.3465305}, technika ta jest niewystarczająco wspierana w funkcjach AWS Lambda.
W ramach pracy zademonstrowano problem ciepłych startów, które uruchamiają niezoptymalizowany kod, co prowadzi do pogorszenia wydajności.

Autorzy zaproponowali platformę IGNITE, która wprowadza pojęcie gorących startów. Są to uruchomienia funkcji z użyciem dodatkowo zoptymalizowanego kodu (poprzez JIT).
W ramach badania, stworzona została alternatywna platforma funkcji, oparta o kontenery Docker. 
Gorące starty były możliwe poprzez ponowne użycie, już uruchomionych wcześniej kontenerów, zawierających zoptymalizowany kod.

Zaproponowana metoda prowadziła do znaczącej redukcji czasu wykonania funkcji (nawet 55-krotnego dla Javy). 
Dodatkowo, widoczny był trend coraz lepszej poprawy wydajności, wraz z kolejnymi uruchomieniami funkcji.

\subsubsection*{GraalVM}

Innym podejściem do optymalizacji funkcji Java w AWS Lambda jest wykorzystanie GraalVM.
GraalVM to wysokowydajny zestaw JDK (ang. Java Development Kit), który może przyspieszyć działanie aplikacji opartych na technologii Java.
Umożliwia kompilację AOT (ang. ahead-of-time) kodu Java do natywnego obrazu, który następnie uruchamiany jest niemal natychmiast oraz zużywa mało zasobów pamięci.

GraalVM to jedna z technik zaproponowanych przez Menendez i Bartlett \cite{menéndez2023performancebestpracticesusing}.
Autorzy wykazali, że użycie tej metody pozwoliło na przyspieszenie zimnych startów o 83\% oraz opóźnienia dla rozgrzanych funkcji o 55\%.
Zaznaczyli oni jednak, że użycie GraalVM może być bardziej skomplikowane w porównaniu do klasycznej maszyny wirtualnej Javy. 
Wynika to z konieczności użycia niestandardowych środowisk uruchomieniowych (ang. custom runtimes) w ramach AWS Lambda, które muszą zostać skonfigurowane przez programistę.

Optymalizacji z użyciem GraalVM oraz różnych frameworków Javy dokonał także Ritzal \cite{ritzal2020optimizing}.
W ramach pracy autor przeanalizował użycie frameworków SpringBoot, Micronaut oraz Quarkus, działających zarówno w ramach klasycznego JVM oraz GraalVM.
Dodatkowo, przetestowana została funkcja bazowa bez użycia żadnego z frameworków.

Wyniki eksperymentu pokazały, że w przypadku użycia zwykłego JVM funkcja bazowa charakteryzowała się najszybszymi zimnymi startami.
Funkcje oparte o frameworki były znacząco wolniejsze. Jednak podczas użycia GraalVM, funkcja oparta o Micronaut posiadała najniższe opóźnienia podczas zimnych startów.
Jednocześnie, charakteryzowała się ona niskim zużyciem pamięci.

\subsubsection*{SnapStart}

SnapStart to funkcjonalność oferowana przez AWS w celu łagodzenia problemu zimnych startów dla funkcji napisanych w Javie. 
Jest to jedna z technik zaproponowanych przez Menendeza i Bartletta w celu optymalizacji funkcji AWS Lambda opartych o Javę \cite{menéndez2023performancebestpracticesusing}. 
Autorzy wykazali, że włączenie tej opcji w testowanym systemie pozwoliło na przyspieszenie zimnych startów o 16\% oraz zmniejszenie opóźnień dla rozgrzanych funkcji o 21\%. 
Zaznaczyli oni jednak, że SnapStart posiada istotne ograniczenia, takie jak brak wsparcia dla architektury ARM64, niestandardowych środowisk uruchomieniowych (ang. custom runtimes), integracji z Amazon EFS czy możliwości połączenia funkcji z VPC.

\subsection{Cechy rozwoju aplikacji w AWS Lambda}\label{chapter:przeglad_literatury_wyniki_cechy_rozwoju}

W podrozdziale rozpatrzono prace poruszające trzecie pytanie badawcze ,,Jakie są cechy rozwoju aplikacji w architekturze bezserwerowej AWS Lambda?''.
Skupiono się aspektach, które muszą zostać poruszone przez programistę podczas tworzenia systemów w architekturze bezserwerowej.
Identyfikacja tych cech pozwoli później na dokładniejszą analizę wpływu konkretnych metod optymalizacji wydajności na te cechy.

\subsubsection*{Zarządzanie wielkością funkcji}

Zbyte duże rozdrobnienie funkcji może powodować wiele pośrednich problemów, które zostały przedstawione przez Nupponen i Taibi \cite{9095731}.
Na bazie doświadczeń programistów, zebrali oni najczęstsze problemy związane z rozwojem oprogramowania w aplikacjach bezserwerowych.
Bezpośrednio wskazują oni na problem zbyt wielu funkcji, który może utrudnić utrzymanie i zrozumienie systemu.
Problemem jest także komunikacja między funkcjami, która może być szczególnie wymagana przy dużej ich liczbie. 
Autorzy podkreślają, że ,,asynchroniczne wywołania do i pomiędzy funkcjami bezserwerowymi zwiększają złożoność systemu''\cite{9095731}, 
a problemem bezpośrednich wywołań jest:
\begin{quote}
     ,,Złożone debugowanie, luźna izolacja funkcji. Dodatkowe koszty, jeśli funkcje są wywoływane synchronicznie, ponieważ musimy płacić za dwie funkcje działające w tym samym czasie.''\cite{9095731}
\end{quote}
W przypadku zbyt dużych funkcji, problematyczne staje się także użycie zbyt dużej liczby bibliotek, co zwiększa ryzyko przekroczenia limitu wielkości artefaktu funkcji.

W kolejnej pracy Taibi kontynuuje analizy w tym zakresie wraz z Kehoe i Poccia \cite{9912641}. 
Wykonali oni badanie ankietowe wśród doświadczonych praktyków pracujących z aplikacjami w architekturach bezserwerowych.
Aż 38.46\% badanych wskazało, że synchroniczne zapytania między funkcjami mają znaczny negatywny wpływ na stan aplikacji.
Także około co 5. badany wskazał, że złą praktyką jest dzielenie tego samego kodu między wieloma funkcjami.
Same funkcje powinny być skupione wyłącznie na pojedynczym zadaniu biznesowym.

Jak wykazali Eismann i inni \cite{eismann2021reviewserverlessusecases} systemy składające się powyżej 5 funkcji są bardzo rzadkie.
W ich badaniach aż 82\% analizowanych przypadków użycia zawierało pięć funkcji lub mniej, a 93\% mniej niż dziesięć. 
Według autorów wynika to potencjalnie z dwóch głównych czynników:
\begin{quote}
    ,,Po pierwsze, bezserwerowe modele aplikacji zmniejszają ilość kodu, który programiści muszą napisać, ponieważ pozwalają im skupić się na logice biznesowej (\dots). Po drugie, wydaje się to wskazywać, że programiści wybierają obecnie raczej dużą ziarnistość dla rozmiaru funkcji bezserwerowych'' \cite{eismann2021reviewserverlessusecases}
\end{quote}
Eismann i inni autorzy podkreślają, że optymalizacja wielkości funkcji jest interesującym tematem do dalszych badań.

W badaniach Leitnera i innych \cite{LEITNER2019340} potwierdzono, że aplikacje bezserwerowe zazwyczaj składają się z niewielkiej liczby funkcji.
Aż 64\% badanych wskazało, że ich aplikacje zawierają od 1 do 10 funkcji, co jest zbliżone do wyników Eismanna.
W kontekście granularności funkcji, najczęściej stosowaną praktyką była drobna granularność, z funkcjami przypisanymi do pojedynczych metod REST (36\% badanych).
Jednak większa liczba funkcji może prowadzić do problemów z zarządzaniem, testowaniem i brakiem odpowiednich narzędzi do monitorowania, co stanowi wyzwanie w praktyce.

\subsubsection*{Integracja z zewnętrznymi usługami}

Rozwój aplikacji w modelu serverless opiera się w dużej mierze na integracji z innymi usługami.
Wynika to znacznie z bezstanowości AWS Lambda, które uruchamiane są na żądanie.
Konieczność integracji podkreślają autorzy Ivanon i Petrova \cite{Ivanov_Petrova_2024}.
Pozwala to budować kompletne systemy w oparciu o funkcje AWS Lambda oraz zarządzane serwisy chmurowe.
Kluczową rolę odgrywają tutaj różnego rodzaju wyzwalacze zdarzeń (na przykład zmiany danych w Amazon S3 czy DynamoDB) i wiadomości przesyłane przez Amazon SNS.
AWS Lambda integruje się także z usługami takimi jak API Gateway, EventBridge czy Step Functions.
Pozwala to budować pełne aplikacje bez własnej infrastruktury.
Autorzy wskazują, że rozwój aplikacji serverless opiera się na łączeniu funkcji z zarządzanymi usługami.

Bezstanowość funkcji AWS Lambda wymusza integrację z zewnętrznymi serwisami, takimi jak bazy danych czy systemy plików.
Ghosh i inni \cite{9027427} zwracają uwagę, że taka komunikacja odbywa się po sieci i może znacząco zwiększać opóźnienia działania aplikacji.
W ich badaniach dostęp do Amazon DynamoDB z funkcji Lambda był prawie czternastokrotnie wolniejszy niż dostęp do lokalnej bazy danych z tradycyjnej aplikacji.
Problem ten narasta w bardziej złożonych systemach, gdzie sieciowe opóźnienia sumują się na ścieżce krytycznej.
Autorzy podkreślają, że problem wynika z samej architektury serverless, gdzie funkcje są uruchamiane w izolacji.
Mimo wad związanych z integracjami z serwisami zewnętrznymi, pozostają one konieczne w rozwoju systemów bezserwerowych.

Na powszechność integracji z serwisami chmurowymi wskazali również Eismann i inni \cite{eismann2021reviewserverlessusecases}.
Dokonali oni analizy przypadków użycia serverless z projektów otwartoźródłowych, białej i szarej literatury oraz konsultacji z ekspertammi.
Wykazali oni, że aplikacje najczęściej korzystały z zewnętrznych rozwiązań przechowywania danych (np. Amazon S3) i baz danych (np. DynamoDB).
Było to odpowiednio 61\% i 47\% analizowanych systemów.
Użycie takich usług było spodziewane ze względu na bezstanowy charakter usług FaaS.
Interesującym wynikiem badania jest, że jedynie 18\% funkcji korzysta z rozwiązań API Gateway (bramka API).

\subsubsection*{Odpowiedni język programowania}

Usługa AWS Lambda oferuje wsparcie dla różnych języków programowania. 
Ich odpowiedni wybór może być kluczowy ze względu na np. wydajność.
Kluczową rolę tej decyzji w cyklu rozwoju oprogramowania wskazali Raza i inni autorzy \cite{raza2021sok}.
Skupili się oni na modelu FaaS z perspektywy programisty, rozwijającego systemu w oparciu o te usługi.
Decyzja o wyborze języka została uznana przez nich jako jednorazowa.
Jej wybór może być podyktowany na przykład kosztem funkcji czy wymaganą wydajnością.
Zaznaczają oni jednak, że zmiana ,,wiązałaby się ze znacznymi kosztami rozwoju i wdrożenia, dlatego deweloper może podjąć taką decyzję tylko raz w cyklu życia aplikacji'' \cite{raza2021sok}.
Zmiana innych parametrów (jak np. wielkości pamięci) ma mniejsze konsekwencje i według programista dokonuje ich bez większego wysiłku.
Mogą one jednak wymagać utworzenia odpowiedniego sposobu ich kontroli i aktualizacji, opartego o analizę ich wpływu na wydajność.

Bardsley, Ryan i Howard \cite{8513710} podkreślają, że wybór języka programowania wpływa bezpośrednio na późniejszy rozwój aplikacji. 
Odpowiednie decyzje mogą ograniczyć opóźnienia i poprawić wydajność systemu. 
Autorzy wskazują, że różne komponenty aplikacji mogą używać różnych języków, w zależności od charakterystyki wywołań funkcji i wymagań wydajnościowcyh. 
W funkcjach reagujących na działania użytkownika (na przykład wykonanie akcji na stronie, jak kliknięcie przycisku) lepiej sprawdzają się języki interpretowane.
Mogą to być na przykład Python czy Node.js, które szybciej inicjalizują kontener. 
Funkcje przetwarzające duże ilości danych lub działające w tle mogą natomiast korzystać z języków kompilowanych, takich jak Java czy C\#.
W takim podejściu każdy element systemu optymalizowany jest indywidualnie. 
Pozwala to zmniejszyć czas odpowiedzi tam, gdzie jest to kluczowe oraz zwiększyć wydajność tam, gdzie operacje są bardziej kosztowne.

\subsubsection*{Testowanie}

Narzędziami, które mogą znacząco poprawić doświadczenie programistów pracujących z technologiami bezserwerowymi są różnego rodzaju frameworki. 
Ich analizy dokonali Skrzypek i Kritikos \cite{8605774}, którzy w swoim przeglądzie wskazali na kluczowe cechy tych narzędzi wspierające zespoły programistyczne.
Zwrócili oni uwagę na wsparcie dla testowania, poprzez lokalne lub zdalne wywołanie wykonanie funkcji.
Tylko niektóre narzędzia w ograniczony sposób wspierały testy jednostkowe (np. framework ,,Fn'' dla Javy i JavaScript oraz framework ,,Serverless'' dla JavaScript).
W narzędziach brakuje jednak wsparcia dla testów integracyjnych.

Chatley i Allerton \cite{10.1145/3377812.3382135} w swojej pracy nad frameworkiem Nimbus również mocno podkreślają, że testowanie jest jednym z głównych wyzwań w trakcie tworzenia aplikacji bezserwerowych.
Autorzy na bazie badań ankietowych i własnych doświadczeń projektowych stwierdzają, że aktualne metody testowania są często trudne, powolne i kosztowne.
Framework Nimbus wprowadza możliwość uruchomienia kompletnej aplikacji w lokalnym środowisku, które symuluje docelową infrastrukturę chmurową.
Pozwala to na wykonanie testów integracyjnych dla funkcji w języku Java.

Testowanie było także cechą podejścia bezserwerowego, wskazanego przez Cavalheiro i Schepke \cite{Cavalheiro202389}.
Dokonali oni implementacji aplikacji z użyciem narzędzi AWS Lambda, Chalice (framework oparty o AWS Lambda) oraz biblioteki Flask (działającej lokalnie, jako podejście tradycyjne).
Zwrócili oni uwagę na o wiele łatwiejsze testowanie z użyciem Flask. 
Umożliwiał on rozwój testów jednostkowych API, co było utrudnione w przypadku modelu serverless.
W przypadku rozwoju AWS Lambda i Chalice trudności sprawiał także proces debuggowania.

Znaczenie testów jednostkowych podkreśla także Leitner i inni autorzy \cite{LEITNER2019340}.
Poprzez wykonane badania ankietowe aż 87\% badanych programistów wykorzystuje ten rodzaj testów w trakcie rozwoju oprogramowania serverless.
Także 55\% uważa, że aktualnie dostępne narzędzia są niewystarczające w obszarach jak testowanie czy wdrożenie.

\subsection{Podsumowanie wyników przeglądu}\label{chapter:przeglad_literatury_wyniki_podsumowanie}

W ramach podrozdziału zaprezentowano wnioski do wyników przeglądu literatury. 
Zostały one przedstawione w kontekście każdego pytania badawczego.

\subsubsection*{Wnioski odnośnie PB1: ,,Jakie są główne czynniki wpływające na wydajność funkcji AWS Lambda?''}

Przegląd literatury pozwolił na identyfikację czynników i ich wpływu na wydajność funkcji AWS Lambda.
Zauważono, że wiele z nich może być w łatwy sposób zaprogramowana przez twórcę funkcji.
Takimi czynnikami są pamięć funkcji czy architektura procesora.
Dzięki naturze usług FaaS konfiguracja infrastruktury jest bardzo prosta, a parametry te mogą być konfigurowane w trakcie całego cyklu rozwoju oprogramowania.
Istotny wpływ na czas działania funkcji ma także język programowania w którym została ona stworzona.
Bardzo szerokim obszarem badań są również zimne starty, które wpływają na czas wykonania funkcji.
Warto jednak zaznaczyć, że na zimne starty wpływ wywiera wiele innych czynników.

Pierwszym ważnym aspektem jest wielkość pamięci funkcji. 
Został on poruszony w pięciu badanych pracach \cite{9284261}\cite{9235063}\cite{8567674}\cite{pawlik2019performanceconsiderationsexecutionlarge}\cite{9946331}.
Popularność badań pod tym względem może wynikać z kilku czynników. 
Wpływ wielkości pamięci na wydajność oprogramowania jest dość oczywisty niezależnie od modelu w jakim ono działa (serwerowe lub bezserwerowe).
Jednak w przypadku architektur serverless, wielkość pamięci jest bezpośrednio powiązana z kosztem działania.
Naturalnie zachęca to do badań nad optymalizacją tego parametru.
Dodatkowo, pamięć to czynnik, który jest bardzo prosto konfigurowalny przez programistę.

W badaniach wykazano, że pamięć oprócz wpływu na czas działania, ma także wpływ na czynniki, które nie są dostępne do konfiguracji (jak wydajność I/O czy sieciowa).
Z analizy tego obszaru wynika, że w przypadku analizy wydajności AWS Lambda, bardzo istotnym elementem jest badanie działania funkcji dla różnych wielkości pamięci.

Mimo wystąpienia wyłącznie w dwóch pracach \cite{10.1145/3631295.3631394}\cite{10.1145/3491204.3543506} architektura procesora także jest interesującym aspektem do uwzględniania w badaniach.
Niemożliwe jest jednak stwierdzenie, że któraś z dostępnych architektur (ARM64 i X86\_64) pozwala na osiągnięcie lepszej wydajności funkcji.
Optymalizacja efektywności AWS Lambda pod względem tego parametru wymaga konkretnej analizy danego przypadku i testów obu opcji.

Popularną strefą badań są także języki programowania dostępne dla AWS Lambda \cite{8605773}\cite{Cordingly2020704}\cite{shrestha2019lambda}.
Autorzy często porównują szybkość języków kompilowanych (jak Java) i interpretowanych (jak Python).
W tym zakresie wyniki często różnią się w zależności od użycia dla ciepłych i zimnych startów.
Mimo badań nad wieloma językami programowania, interesujący jest jednak brak reprezentacji innych niż Java języków działających w JVM.
Mogą to być na przykład Kotlin, Scala czy Groovy.

Języki wywodzące się z Javy (jak Kotlin) mogą być interesującym obszarem dalszych badań w kontekście wydajności.
Każdy z tych języków posiada specyficzne aspekty, które mogą wpłynąć na wydajność.
Dodatkowo, ich kompatybilność z Javą może usprawnić ich użycie przez istniejące już zespoły zaznajomione z tym językiem.

Bardzo szerokim obszarem badań są także zimne starty.
Polegają one na inicjalizacji infrastruktury potrzebnej do uruchomienia kodu wywołanej funkcji, co powoduje dodatkowe opóźnienia. 
Wpływają one bezpośrednio na wydajność funkcji, a wpływ na zimne starty mają także pozostałe czynniki przedstawione we wnioskach.
Wynika z tego, że eksperymenty z użyciem metod poprawy wydajności powinny uwzględniać zarówno ciepłe, jak i zimne starty.

\subsubsection*{Wnioski odnośnie PB2: ,,Jakie są istniejące metody optymalizacji wydajności funkcji AWS Lambda działających w ekosystemie Java?''}

Dokonanie przeglądu literatury pozwoliło na poznanie aktualnego stanu wiedzy w kontekście metod optymalizacji wydajności.
Ważnym elementem poprawy wydajności jest zmniejszanie wielkości artefaktu, które może być dokonywane w całym procesie rozwoju oprogramowania.
Składa się na niego wiele czynników (jak np. użyte biblioteki), co daje szeroką możliwość optymalizacji tego aspektu.
W literaturze zawarte występują także metody, które nie są specyficzne dla Javy jak odpowiedni rodzaj wdrożeń (ZIP lub Docker) czy pingowanie.
W ramach odpowiedzi na pierwsze pytanie badawcze wykazano znaczący wpływ języka programowania na wydajność, zatem ciekawym kierunkiem w literaturze jest propozycja użycia języków alternatywnych do Javy.
Nie istnieje jednak wiele badań na temat bardzo skutecznych metod opierających się o usprawnienia kompilacji (jak techniki JIT czy GraalVM) oraz SnapStart.

Redukcja wielkości artefaktu może być dokononana poprzez różne techniki jak redukcja nieużywanego kodu, czy zmniejszenie liczby bibliotek.
Jest ona wykonywana w celu poprawy wydajność oraz uniknięcia sytuacji przekroczenia limitu wielkości funkcji.
Jednak ważnym aspektem tej metody jest stronienie od nadmiernego podziału funkcji.
Zbyt duża ich liczba może prowadzić do wydłużenia czasu działania, ze względu na kosztowne zapytania między funkcjami.
Dlatego każda optymalizacja powinna być dokładnie analizowana w zależności od optymalizowanego komponentu.

Znaleziono wyłącznie pojedynczą pracę porównującą wdrożenia oparte o ZIP i obrazy Docker \cite{9860368}.
Na jej bazie możemy jednak stwierdzić o skuteczności użycia ZIP jako metody optymalizacji sprawności AWS Lambda dla technologii Java.
Technika ta może być jednak użyta w kombinacji z nowymi metodami.

Bardzo interesującym kierunkiem badań są techniki oparte o użycie innych języków.
Na ten temat zawarto w wynikach dwie prace.
Pierwsze z nich, proponuje użycie JavaScript jako całkowite zastąpienie Javy \cite{FerreiraDosSantos2023}.
Metoda ta jednak nie jest rozwiązaniem dla programistów Javy i trudno nazwać ją metodą optymalizacji wydajności w ekosystemie Java.
Wskazuje ona jednak wartościowy trend dla użycia odpowiednich języków w konkretnych elementach systemu.

Druga praca proponuje migracje aplikacji monolitycznych z Javy do języka JavaScript \cite{8844428}.
Stworzone narzędzie pozwoliło na migrację starych systemów do architektury bezserwerowej.
Technika ta nie pozwala jednak na rozwój aplikacji opartych o Javę, a docelowo zastępuje ją JavaScriptem.
Jednak ponownie, praca ukazuje ciekawy obszar do dalszych badań.

Bardzo skuteczne okazują się metody optymalizujące kompilację i uruchomienie kodu Java.
Techniki jak JIT \cite{10.1145/3458336.3465305} czy GraalVM \cite{menéndez2023performancebestpracticesusing}\cite{ritzal2020optimizing} pozwalają na znaczące zmniejszenie opóźnień.
Jednak liczba prac jest niewielka w porównaniu z skutecznością tego rozwiązania.
Dodatkowo, prace skupiały się wyłącznie na wpływie na czas działania funkcji pomijając inne czynniki (np. czas budowania artefaktu).
Z tego względu to pole może wymagać dokładniejszej analizy.

Niespodziewanym było znalezienie wyłącznie pojedynczego badania w kontekście SnapStart \cite{menéndez2023performancebestpracticesusing}.
Metoda ta jest dostępna od roku 2022, a jej wdrożenie przez programistę jest stosunkowo proste.
Sama technika nie niesie za sobą także dodatkowych kosztów, a pozwala na zmniejszenie wpływu zimnych startów na efektywność AWS Lambda.
Jest ona także dostępna dla innych języków działających w JVM, co nie zostało zbadane.
Zatem obszar ten może wykazywać potencjał do dalszych badań.

\subsubsection*{Wnioski odnośnie PB3: ,,Jakie są cechy rozwoju aplikacji w architekturze bezserwerowej AWS Lambda?''}

W trakcie rozwoju aplikacji w architekturze bezserwerowej od programisty wymagane są konkretne decyzje i działania.
W celu optymalizacji architektury istotne jest tworzenie funkcji odpowiedniego rozmiaru (co także znacząco wpływa na wydajność).
Systemy często oparte są o dodatkowe serwisy (np. bazodanowe), co dodatkowo utrudnia kolejny aspekt tej architektury, czyli testowanie.
W przypadku testowania funkcji bezserwerowych dominują testy jednostkowe, a testy integracyjne są utrudnione.
Ważnym elementem tworzenia nowego komponentu jest wybór języka, a jego twórca musi dostosować tę decyzję do konkretnych wymagań.

Tworzenie systemów w architekturze opartej o AWS Lambda cechuje się odpowiednią kompozycją tych funkcji.
Ich wielkość oprócz znaczącego wpływu na wydajność ma także wpływ na rozwój systemu \cite{9095731}\cite{9912641}.
Wynika z tego, że w badania nad metodami optymalizacji wydajności muszą zostać przeprowadzone w kontekście odpowiednich funkcji.
Powinny być one nie za duże, a skupione na konkretnym zadaniu biznesowym.

Kluczową rolę w modelu bezserwerowym odgrywają zewnętrzne serwisy. 
Funkcje FaaS w swojej naturze są bezserwerowe, dlatego bardzo istotne jest użycie usług jak bazy danych \cite{eismann2021reviewserverlessusecases}.
Element ten jest istotny ze względu na potrzebę posiadania odpowiednich narzędzi do takich integracji.
Mogą to być różne narzędzia SDK (ang. Software Development Kit), na przykład od integracji z usługami AWS.
Z tego względu metody optymalizacji nie mogą ograniczać tak krytycznych zależności.

Fundamentalnym wyborem dla tworzonej funkcji AWS Lambda jest język programowania.
W ramach poprzednich pytań badawczych wykazano ich znaczący wpływ na wydajność.
Ma on także znaczny udział w rozwoju oprogramowania opartego o model serverless.
Autorzy prac zaznaczają, że musi być on odpowiedni dla konkretnych wymagań biznesowych \cite{raza2021sok}\cite{8513710}.
Dlatego interesującym obszarem badań mogą być metody, które ułatwią użycie różnych języków w ramach AWS Lambda.

Trudności w procesie progamistycznym aplikacji opartych o AWS Lambda przynosi testowanie oprogramowania.
Aktualnie testy funkcji opierają się głównie o testy jednostkowe, które są wspierane przez różne frameworki \cite{8605774}.
Brakującym elementem są testy integracyjne, które w tym modelu są mocno skomplikowane.
Dlatego metody poprawy wydajności nie powinny utrudniać testowania, które jest już teraz jest wyzwaniem.

\subsection{Zidentyfikowane luki badawcze}\label{chapter:przeglad_literatury_wyniki_luki}

Poprzez zrealizowanie przeglądu literatury możliwe było nie tylko poznanie aktualnego stanu wiedzy w kontekście AWS Lambda ale także identyfikacja luk badawczych.
Obszary te stanowią uzasadnienie wyboru metod optymalizacji, które zostaną zawarte w pracy.
Dodatkowo, pozwoli to na stworzenie warunków badania, które przeanalizują wybrane metody pod kątem nowych aspektów.
oraz sposobu ich analizy, co zostało zawarte w pracy.

\subsubsection*{Brak reprezentacji alternatywnych języków ekosystemu Java w AWS Lambda}

W ramach przeglądu literatury wykazano, że istniejące badania koncentrują się niemal wyłącznie na samym języku Java.
Istotną luką jest brak prac badawczych, które poruszałyby języki alternatywne do Javy.
Mogą to być popularne języki jak Kotlin, Scala czy Groovy.
Ważna jest tutaj rosnąca popularność Kotlina, który według ankiety StackOverflow z 2024 roku jest używany przez blisko 10\% profesjonalnych programistów \cite{stackoverflow_survey_2024}.
Języki alternatywne oprócz samych aspektów języka, dostarczają także bardzo szeroki wachlarz narzędzi.
Zawarcie tych elementów w badaniach może być znacznym czynnikiem, który spopularyzuje języki jak Kotlin w usłudze AWS Lambda.

Przypadek języka Kotlin może być szczególnym dla AWS Lambda.
Po pierwsze, stanowi on bezpośrednią alternatywę dla Javy, który zapewnia inną jakość pracy dla programistów oraz interoperacyjność z Javą.
Oferuje także projekt Kotlin Multiplatform, który może pozwolić na wykorzystanie kodu Kotlin w innych środowiskach wykonawczych niż maszyna wirtualna Java.
Może na to pozwolić JavaScript (poprzez translację z użyciem Kotlin/JS), a także natywne pliki uruchomienione (poprzez Kotlin/Native).
W ramach przeglądu zawarto pewien trend użycia JavaScript zamiast Javy.
Metody jak Kotlin/JS mogą być innowacyjne ze względu na możliwość uruchamiania kodu funkcji w środowisku Node.js przy jednoczesnym rozwijaniu oprogramowania w języku z rodziny Java.
Dodatkowo, Kotlin/Native może być alternatywną metodą dla GraalVM, który także nie jest dostatecznie reprezentowany w badaniach.

\subsubsection*{Ograniczone i niekompletne analizy metod SnapStart i GraalVM}

W ramach przeglądu literatury zauważono, że metody SnapStart oraz GraalVM są wskazywane jako skuteczne metody optymalizacji.
Poświęcono im jednak niewielką liczbę badań \cite{menéndez2023performancebestpracticesusing}\cite{ritzal2020optimizing}.
Dodatkowo, skupiały się one tylko na podstawowych metrykach jak czas działania funkcji podczas ciepłych i zimnych startów.
Nie analizowały one także skuteczności metod w zależności od rozmiaru pamięci, która ma znaczący wpływ na wydajność samej funkcji.
Pomijają one także niemal całkowicie badanie wpływu tych metod na proces rozwoju oprogramowania.
Wpływ ten jest jednak istotnym aspektem w momencie wyboru metody optymalizacji wydajności.

\subsubsection*{Niewystarczająca analiza wydajności funkcji AWS Lambda}

Przegląd literatury wykazał, że istniejące badania skupiają swoją ocenę wydajność jedynie poprzez czas zimnych i ciepłych startów.
Są one traktowane rozłącznie, jako dwie niezależne od siebie metryki.
Takie podejście jest poprawne, jednak niewystarczające aby w pełni odwzorować realne użycie funkcji AWS Lambda.
W praktyce każda funkcja podlega obu typom uruchomień.
Ich proporcje zależą od typu funkcji, która może być bardziej aktywna (z dominacją ciepłych startów) lub mało aktywna (gdzie częściej występują zimne starty).
Dodatkowo, brak analizy obu tych aspektów w pewnym połączeniu, uniemożliwia realną ocenę kosztów działania funkcji.
Z tego powodu, istnieje potrzeba stworzenia metryk, które odzwierciedlą realne działanie funkcji AWS Lambda.
Mogą to być metryki agregujące oba typy startów oraz oparte o koszt działania funkcji, który jest ważną cechą usług bezserwerowych.

\subsubsection*{Brak analizy wpływu metod optymalizacji na proces rozwoju oprogramowania}

Jedynie pojedyncza praca z przeglądu literatury poruszała wpływ metody optymalizacji wydajności na proces rozwoju oprogramowania \cite{10.1145/3377812.3382135}.
Element ten jest jednak kluczowy z punktu widzenia programisty rozwijającego system używający usługi AWS Lambda.
Sama wydajność nie powinna być jedynym czynnikiem decydującym o wyborze metody optymalizacji.
W przypadku metody, która znacznie ogranicza możliwości rozwoju oprogramowania, oszczędności za działanie funkcji mogą być iluzoryczne.
Ukrytymi kosztami może być wtedy dłuższy czas rozwoju nowych funkcji lub dłuższy proces tworzenia artefaktów.
Bardzo istotny jest zatem obszar analizy metod w kontekście wybranych aspektów procesu rozwoju oprogramowania systemów bezserwerowych.
Aspekt ten powinien zostać zawarty w pracy, aby ułatwić programistom podjęcie świadomej decyzji o metodzie optymalizacji wydajności. 
