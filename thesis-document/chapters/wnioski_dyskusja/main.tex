\chapter{Wnioski i dyskusja}\label{chapter:wnioski_dyskusja}

\section{Odpowiedzi na pytania badawcze}\label{chapter:odpowiedzi_na_pytania_badawcze}

W ramach podrozdziału udzielono odpowiedzi na pytania badawcze sformułowane w pracy. 
Odpowiedzi bazowały na wynikach opisanych w Rozdziale \ref{chapter:wyniki} oraz przypadku trzeciego pytania badawczego także na cechach metod opisanych w Rozdziale \ref{chapter:wybrane_metody_optimalizacji}.

\subsection*{Odpowiedź na pierwsze pytanie badawcze}

Pierwsze pytanie badawcze postawione w pracy brzmi: ,,Które metody optymalizacji pozwalają na najlepszą poprawę ogólnej wydajności funkcji AWS Lambda w ekosystemie Java?''.
W celu odpowiedzi w pracy zawarto cztery różne kryteria: czas działania funkcji podczas ciepłych startów, czas działania funkcji podczas zimnych startów, współczynnik wydajności funkcji oraz koszt działania funkcji.
Badania pod ich kątem wykazały, że efektywność poszczególnych metod jest bardzo zależna od konkretnego przypadku użycia (np. często lub rzadko używane funkcje) oraz wielkości pamięci.

W przypadku funkcji o wysokiej aktywności (czyli z dominacją ciepłych startów), żadna z badanych metod nie wykazała poprawy wydajności dla każdej z badanych wielkości pamięci.
Dla czasu ciepłych startów widoczny jest negatywny wpływ usługi SnapStart (zarówno w przypadku Javy, jak i Kotlina) oraz zastosowania języka interpretowanego (w przypadku użycia Kotlin/JS).
Metody bazujące na kompilacji kodu do natywnych plików binarnych są najbardziej obiecujące.
Są to platformy GraalVM oraz Kotlin/Native, których efektywność możemy rozpatrzyć pod kątem małych i dużych rozmiarów pamięci.
Dla małych rozmiarów pamięci (128 MB oraz 256 MB) Kotlin/Native zapewnił niższe średnie czasy wykonania niż klasyczna funkcja Java JVM, o odpowiednio 49\% i 35\%.
Nie były to jednak różnice znaczne, gdyż nie zostały potwierdzone wynikami testu statystycznego.
GraalVM pozwolił na znaczną poprawę średniego czasu wykonania dla pamięci 128 MB (o 60\%), jednak w przypadku pamięci 256 MB cechował się wydłużeniem średniego czasu odpowiedzi.
Dla pamięci 512 MB obie metody cechowały się dłuższym średnim czasem procesowania niż klasyczna funkcja Java JVM, jednak GraalVM był znacznie szybszy (pogorszenie o 12\%, w porównaniu z 231\% dla Kotlin/Native).
W przypadku większych pamięci, czyli 1024 MB i 2048 MB, GraalVM skrócił czas średni procesowania o odpowiednio 40\% i 23\%.
Istotny jest także fakt, że metody bazujące na kompilacji do natywnych plików binarnych cechowały się pewną niestabilnością.
Objawiała się ona wysokimi wartościami odchyleń standardowych (w przypadku GraalVM) lub nagłymi skokami wydajności wraz z zmianą pamięci (w przypadku Kotlin/Native).

Zatem dla funkcji AWS Lambda o wysokiej aktywności i wysokim ponownym użyciu rozgrzanych już instancji najbardziej uniwersalnym wyborem będzie użycie GraalVM.
Jednak w przypadku chęci użycia mniejszej pamięci, Kotlin/Native będzie sposobem bardziej stabilnym (ze względu na dużo niższe odchylenia standardowe), który zapewni pewną poprawę czasu działania.
Dodatkowo, metoda ta wykazuje bardzo wysoką poprawę efektywności kosztowej.
Dla rozmiarów pamięci 128 MB i 256 MB Kotlin/Native pozwolił na uzyskanie najniższych kosztów, zatem może to być istotny czynnik w wyborze metod optymalizacji przez zespoły programistyczne.

Analizując przypadek funkcji mniej aktywnych (czyli z większym udziałem zimnych startów), metody wykazywały inne właściwości niż dla ciepłych startów.
Poprzez ocenę współczynnika wydajności funkcji (WWF) można stwierdzić, że większość metod pozwoliło na pewną poprawę wydajności.
Jedynie dla mniejszych pamięci (128 MB, 256 MB) metody jak SnapStart i Kotlin prowadziły do pogorszenia wydajności (oprócz użycia Javy z aktywnym SnapStart i pamięci 256 MB).
Dla pamięci wielkości 128 MB, 256 MB ponownie najbardziej skuteczne okazały się metody oparte o kompilację do natywnych plików binarnych.
Szczególną uwagę przykuwa jednak użycie Kotlin/Native, który zapewnił znacznie większy wzrost wydajności mierzony przez WWF niż GraalVM.
Wykazał się on najbardziej efektywną metodą dla funkcji o pamięci 128 MB, a także dla rozmiaru 256 MB, gdzie wzrost wydajności był kolosalny.
Kotlin/Native osiągnał trzykrotnie większą wartość współczynnika niż GraalVM, który był drugą najbardziej skuteczną metodą.
Czas uruchomienia funkcji dla pamięci 128 MB i 256 MB był także najniższy dla Kotlin/Native.
Zostanie to omówione szczegółowo w ramach odpowiedzi na drugie pytanie badawcze.
Dodatkowo, w ramach tych wartości pamięci możliwe było uzyskanie najniższych kosztów działania funkcji wśród wszystkich badanych konfiguracji.
Zostało to otrzymane dzięki użyciu kompilacji Kotlina do natywnych plików binarnych.
Wszystkie te aspekty wskazują na bardzo wysoką skuteczność tej metody w zakresie niewielkich rozmiarów pamięci (128 MB, 256 MB).

Dla mniej aktywnych funkcji o rozmiarze pamięci od 512 MB do 2048 MB, trzy z badanych metod wykazały istotną skuteczność.
Były to Kotlin/JS, Kotlin/Native oraz GraalVM.
Dla pamięci 512 MB, metody wykazały zbliżoną skuteczność pod względem współczynnika wydajności funkcji (przy czym GraalVM był najbardziej wydajny).
Wraz z wzrostem pamięci rosła wydajność wszystkich trzech metod, jednak istotny jest tutaj wzrost wydajności dla funkcji używających Kotlin/Native.
Dla rozmiaru pamięci 1024 MB Kotlin/Native uzyskał najwyższą wartość współczynnika, o około 10\% wyższą niż kolejny GraalVM.
Jednak dla rozmiaru 2048 MB, różnica ta zdecydowanie zwiększyła się, gdzie Kotlin/Native osiągnął wynik o około 74\% wyższy od GraalVM.
Wartym uwagi jest także bardzo wysoka wydajność Kotlin/Native w konfiguracji pamięci 256 MB, gdzie było to połączenie o drugiej najwyższej wartości współczynnika.
Dodatkowo, Kotlin/Native cechował się wysoką efektywnością kosztową, gdzie pozwolił na uzyskanie niższego kosztu działania niż GraalVM w prawie wszystkich z badanych rozmiarów pamięci (oprócz 512 MB).

Podsumowując, niemożliwe jest wybranie jednej najbardziej efektywnej metody optymalizacji wydajności dla wszystkich przypadków użycia funkcji AWS Lambda w ekosystemie Java.
Dla funkcji o bardzo wysokiej aktywności, GraalVM byłby najbardziej uniwersalną techniką, która jednak nie jest skuteczna dla każdej konfiguracji pamięci.
Podczas użycia funkcji o mniejszej aktywności i wyższym znaczeniu zimnych startów, bardzo sprawną techniką jest Kotlin/Native, który cechuje się także dobrą efektywnością kosztową, w porównaniu z alternatywnym GraalVM.
Wartym uwagi jest jednak przypadek funkcji o niewielkim rozmiarze pamięci (128 MB, 256 MB) i używających Kotlin/Native.
W ramach tych funkcji metoda ta pozwala na poprawę wydajności w ramach większości z badanych aspektów.
Osiąga ona najniższy czas działania dla zimnych startów oraz najwyższy współczynnik wydajności funkcji w ramach tych rozmiarów pamięci.
W przypadku ciepłych startów, jej efektywność jest wyższa lub podobna w porównaniu z funkcją Java JVM.
Wszystko to jest możliwe przy jednocześnie najniższych z otrzymanych w badaniu kosztów działania.
Z tego względu Kotlin/Native powinien być rozważany w przypadku decyzji o wyborze metody optymalizacji wydajności.

\subsection*{Odpowiedź na drugie pytanie badawcze}

Drugim pytaniem badawczym sformułowane w pracy jest: ,,W jakim stopniu wybrane metody optymalizacji redukują czas zimnego startu funkcji Java w AWS Lambda?''.
W celu udzielenia odpowiedzi w badaniu zawarto pomiar czasu działania funkcji podczas zimnych startów.
Dokładne wyniki zostały przedstawione w Rozdziale \ref{chapter:results_cold_start}.
W tym przypadku skuteczność metod była generalnie wysoka.
Jedynym wyjątkiem było zastosowanie funkcji SnapStart w połączeniu z Kotliniem, co wpłynęło negatywnie na efektywność podczas zimnych startów.
Dla Javy użycie SnapStart zredukowało czas zimnych startów od 10\% (dla pamięci 128 MB) do 55\% (dla pamięci 1024 MB).
Użycie Kotlina także było skuteczne, choć tylko dla większych rozmiarów pamięci (512 MB, 1024 MB, 2048 MB), gdzie poprawa wynosiła odpowiednio 28\%, 54\% oraz 65\%.

Najbardziej wydajnymi metodami były te oparte o kompilacje do natywnych plików binarnych (GraalVM, Kotlin/Native) oraz używające języków interpretowanych (Kotlin/JS).
Porównując GraalVM oraz Kotlin/JS, pierwsza metoda była średnio bardziej skuteczna dla małych pamięci (128 MB, 256 MB), jednak nie była to znaczna różnica.
Zapewniła ona poprawę średniego czasu odpowiedzi o odpowiednio 78\% i 81\%, gdy dla Kotlin/JS wynik ten wynosił odpowiednio 75\% i 80\%.
W przypadku pamięci 512 MB, 1024 MB i 2048 MB GraalVM zredukował czas odpowiedzi o odpowiednio 82\%, 82\% i 80\%, a Kotlin/JS zredukował czas odpowiedzi o odpowiednio 84\%, 84\% i 86\%.
Na bazie testów statystycznych wykazano, że dla pamięci 1024 MB przewaga Kotlin/JS jest znaczna, dla pozostałych już mniejsza.

Najefektywniejszą metodą okazał się jednak Kotlin/Native.
Dla każdej z badanych wielkości pamięci pozwolił on na największą redukcję czasu działania.
Wynosiła ona od 88\% (dla pamięci 128 MB) do 93\% (dla pamięci 1024 MB i 2048 MB) w porównaniu z bazową funkcją Java JVM.
Dla rozmiarów pamięci 128 MB i 256 MB, funkcje z Kotlin/Native charakteryzowały się zimnymi startami odpowiednio o około 45\% i 51\% niższymi w porównaniu do GraalVM.
Z kolei dla konfiguracji z pamięcią 512 MB, 1024 MB i 2048 MB, czasy zimnego startu były odpowiednio o 50\%, 48\% i 48\% krótsze niż dla Kotlin/JS.

\subsection*{Odpowiedź na trzecie pytanie badawcze}

Trzecim pytaniem badawczym, które zostało uwzględnione w pracy, jest: ,,Jakie kompromisy w procesie rozwoju oprogramowania wiążą się implementacją poszczególnych metod optymalizacji wydajności funkcji Java w AWS Lambda?''.
Wpływ na rozwój oprogramowania to istotny czynnik, który powinien być wzięty pod uwagę przez zespoły programistyczne podczas wyboru odpowiedniej metody optymalizacji wydajności.
Znaczenie może zostać ocenione pod kątem różnych kryteriów jak: czas budowy artefaktu, jego wielkość, dostępność narzędzi SDK (ang. Software Development Kit) czy kosztem działania.
W ramach oceny wszystkie metody są porównywane do bazowej implementacji Java działającej z użyciem JVM.

Podczas użycia usługi SnapStart programista dalej wykorzystuje ten sam artefakt i platformę, jak w funkcji bazowej.
Zatem nie ma ona wpływu na czynniki jak czas budowy, wielkość artefaktu czy dostępność SDK.
Istotny jest jednak wpływ na koszt funkcji, który był wyższy dla każdego z badanych rozmiarów pamięci oraz dla obu języków (Java i Kotlin).
W trakcie tworzenia systemów korzystających z SnapStart programista powinien uwzględnić jednak kilka aspektów związanych z stanem funkcji (co zostało opisane w Rozdziale \ref{chapter:snapstart}).
Wpływa to na połączenia sieciowe funkcji (szczególnie ważne podczas integracji z innymi serwisami) oraz generowania wartości unikalnych (jak identyfikatory).
Wynika to z sposobu działania SnapStart, który tworzy migawki stanu funkcji, wykorzystywane później w wielu wywołaniach.

Użycie GraalVM niesie za sobą pewne kompromisy dla rozwoju oprogramowania, które są jednak umiarkowane.
Istotny jest wpływ na tworzony artefakt funkcji, którego budowa znacznie wydłuża się (w porównaniu z tworzeniem klasycznego pliku JAR), a rozmiar zwiększa się.
GraalVM poprzez zmianę w sposobie kompilacji zmienia także zachowanie tworzonego kodu.
Wpływa to na np. mechanizm refleksji, którego użycie wymaga od programisty dodatkowej pracy związanej z odpowiednim oznaczenim metod i klas.
Możliwe jest jednak dalej użycie AWS SDK, by integrować funkcje z innymi usługami.
Mimo poprawy wydajności kosztów, ważnym aspektem jest jednak wzrost kosztów dla większych pamięci funkcji (1024 MB, 2048 MB).

Poprzez użycie języka Kotlin programista zyskusje szeroke możliwości związane z projektem Kotlin Multiplatform.
Język ten jest zbliżony do Javy i w przypadku użycia razem z JVM, nie niesie za sobą znacznych kompromisów w procesie rozwoju oprogramowania.
Należy jedynie wspomnieć o negatywnym wpływie na koszt działania, który średnio był wyższy niż dla analogicznych funkcji w Javie.
Większy wpływ na tworzenie aplikacji mają metody oparte o Kotlin/JS oraz Kotlin/Native.
Pierwszym z nich jest czas tworzenia artefaktu, który jest wyższy dla obu metod (jednak znacznie wyższy dla Kotlin/Native).
Warto jednak zaznaczyć, że rozmiar artefaktu był mniejszy dla wszystkich funkcji opartych o Kotlin, w porównaniu z klasyczną Javą.
Pewnym kompromisem są także integracje z zewnętrznymi serwisami, które mogą być utrudnione ze względu na dostępność AWS SDK.
Jest ono niedostępne dla Kotlin/Native oraz dostępne pośrednio dla Kotlin/JS, gdzie wymaga dodatkowej integracji.
Ograniczeniami dla tych sposobów są także ograniczone możliwości użycia mechanizmu refleksji, podobnie jak w przypadku GraalVM.

\section{Zagrożenia trafności wyników}\label{chapter:zagrozenia_trafnosci_wynikow}

% TODO: wstęp czemu te typy

\subsection*{Trafność wnioskowania}

Istotnym zagrożeniem dla wniosków wyciągniętych na podstawie eksperymentu może być niska moc statystyczna wyników. 
Badanie opierało się na dużej, jednak określonej, liczbie wywołań dla każdej z funkcji i konfiguracji. 
Mimo przeprowadzenia wielu testów, zmienność środowiska chmurowego i potencjalne losowe zmiany w czasach odpowiedzi mogły wpłynąć na wyniki. 
W ramach badania starano się zminimalizować ten wpływ, poprzez kilka czynników. 
Był to brak integracji z zewnętrznymi serwisami (np. API Gateway, DynamoDB), przeprowadzenie analizy statystycznej oraz wydłużenie czasu eksperymentu poprzez oczekiwanie pomiędzy kolejnymi wywołaniami.
Dodatkowo, eksperyment został wykonany w pojedynczym regione (eu-central-1) oraz w godzinach nocnych, gdzie obciążenie infrastruktury AWS jest mniejsze \cite{10.1145/3491204.3543506}.
Aby jeszcze bardziej zmniejszyć to ryzyko możliwe byłoby jeszcze większe wydłużenie czasu eksperymentu i regularne wywoływanie funkcji przez na przykład kilka dni.
Proces ten wymaga jednak znacznie dłuższego badania, które przekracza zakres aktualnej pracy.

\subsection*{Trafność wewnętrzna}

Potencjalnym czynnikiem, który mógłby negatywnie wpłynąć na trafność wewnętrzną eksperymentu jest dynamiczna natura środowiska chmurowego.
Inne procesy działające w tym samym czasie w infrastrukturze AWS i niezwiązane z badaniem, mogły teoretycznie wpłynąć na wydajność badanych funkcji (np. poprzez chwilowe obciążenie sieci lub współdzielonych zasobów).

Innym aspektem są potencjalne błędy lub różnice w implementacji funkcji.
W przypadku języka Kotlin postarano się zminimalizować ten wpływ poprzez współdzielenie kodu między wszystkiem metody oparte o ten język.
Jednak poszczególne metody stosują różne biblioteki i frameworki, których wybór jest ograniczony poprzez daną metodę (np. wsparcie dla kompilacji natywnej).
Sama ich analiza i porównanie może być jednak wymagające, co stanowi potencjalny kierunek rozwoju pracy.

\subsection*{Trafność konstruktu}

Trafność konstruktu mogła być zagrożona ze poprzez przyjęte w pracy miary wydajności oraz kompromisów w procesiu rozwoju oprogramowania.
Wydajność została zmierzona za pomocą czasu wykonania (ciepłe i zimne starty), kosztu oraz autorskiego współczynnika WWF.
Trafność ta może być jednak zagrożona ze względu na zróżnicowane proporcje ciepłych i zimnych startów w systemach opartych o funkcje AWS Lambda.
W ramach badania przyjęto jedynie pojedyncze ich proporcje, zatem istnieje ryzyko niezawarcia w badaniu szczególnych przypadków użycia badanej usługi.

Wpływ na proces rozwoju oprogramowania oceniono na podstawie czasu budowy artefaktu, jego wielkości, dostępności AWS SDK, kosztu działania funkcji oraz innych czynników opisanych w Rozdziale \ref{chapter:wybrane_metody_optimalizacji}.
Opis procesu rozwoju oprogramowania jedynie w tych aspektach jest jednak uproszczony.
Niesie to zatem ryzyko nieprawidłowej oceny konkretnych metod.
Aby zapewnić lepszą jakość badania pod tym względem, możliwa byłaby analiza konkretnych projektów korzystających z danych metod.
Jest to jednak utrudnione ze względu na niewielką popularność części z nich.

\subsection*{Trafność zewnętrzna}

W badaniu zaimplementowano bardzo ogólne zadanie obliczeniowe czyli mnożenie macierzy oraz powstrzymano się od integracji zewnętrznych usług.
Miało to na celu ograniczenie wpływu zewnętrznych czynników na czas działania funkcji.
Jak wykazano w przeglądzie literatury dodatkowe serwisy są jednak bardzo częstym elementem architektur bezserwerowych.
Z tego powodu zaimplementowane operacje mogą być niewystarczające aby przełożyć wyniki badania na przypadki innych systemów.
Systemy te mogą opierać się np. o wywołania dodatkowych API, które nie zostały uwzględnione w eksperymencie.

AWS Lambda zapewnia istotną abstrakcję na infrastrukturę chmurową, która jednak dalej odgrywa istotną rolę w wydajności funkcji.
Eksperyment przeprowadzono w jednym regionie AWS i dla jednej architektury procesora (x86-64). 
Jak wykazał przegląd literatury, wydajność może różnić się w zależności od regionu geograficznego oraz architektury (np. ARM64). 
Zatem wyniki mogą nie być w pełni reprezentatywne dla wszystkich dostępnych konfiguracji.
