\chapter{Wnioski i dyskusja}\label{chapter:wnioski_dyskusja}

\section{Odpowiedzi na pytania badawcze}\label{chapter:odpowiedzi_na_pytania_badawcze}

W ramach podrozdziału udzielono odpowiedzi na pytania badawcze sformułowane w pracy. 
Odpowiedzi bazowały na wynikach opisanych w Rozdziale \ref{chapter:wyniki} oraz przypadku trzeciego pytania badawczego także na cechach metod opisanych w Rozdziale \ref{chapter:wybrane_metody_optimalizacji}.

\subsection*{Odpowiedź na pierwsze pytanie badawcze}

Pierwsze pytanie badawcze postawione w pracy brzmi: ,,Które metody optymalizacji pozwalają na najlepszą poprawę ogólnej wydajności funkcji AWS Lambda w ekosystemie Java?''.
W celu odpowiedzi w pracy zawarto cztery różne kryteria: czas działania funkcji podczas ciepłych startów, czas działania funkcji podczas zimnych startów, współczynnik wydajności funkcji oraz koszt działania funkcji.
Badania pod ich kątem wykazały, że efektywność poszczególnych metod jest bardzo zależna od konkretnego przypadku użycia (np. często lub rzadko używane funkcje) oraz wielkości pamięci.

W przypadku funkcji o wysokiej aktywności (czyli z dominacją ciepłych startów), żadna z badanych metod nie wykazała uniwersalnej poprawy wydajności dla każdej z badanych wielkości pamięci.
Dla czasu ciepłych startów widoczny jest negatywny wpływ usługi SnapStart (zarówno w przypadku Javy, jak i Kotlina) oraz zastosowania języka interpretowanego (w przypadku użycia Kotlin/JS).
Metody bazujące na kompilacji kodu do natywnych plików binarnych są najbardziej obiecujące.
Są to platformy GraalVM oraz Kotlin/Native, których efektywność możemy rozpatrzyć pod kątem małych i dużych rozmiarów pamięci.
Dla małych rozmiarów pamięci (128 MB oraz 256 MB) Kotlin/Native zapewnił niższe średnie czasy wykonania niż klasyczna funkcja Java JVM, o odpowiednio 49\% i 35\%.
Należy jednak podkreślić, że różnice te nie były istotne statystycznie dla tych rozmiarów pamięci, co wskazuje, że samo skrócenie czasu ciepłego startu nie jest główną przewagą Kotlin/Native.
GraalVM pozwolił na znaczną poprawę średniego czasu wykonania dla pamięci 128 MB (o 60\%), jednak w przypadku pamięci 256 MB cechował się wydłużeniem średniego czasu odpowiedzi.
Dla pamięci 512 MB obie metody cechowały się dłuższym średnim czasem procesowania niż klasyczna funkcja Java JVM, jednak GraalVM był znacznie szybszy (pogorszenie o 12\%, w porównaniu z 231\% dla Kotlin/Native).
W przypadku większych pamięci, czyli 1024 MB i 2048 MB, GraalVM skrócił czas średni procesowania o odpowiednio 40\% i 23\%.
Istotny jest także fakt, że metody bazujące na kompilacji do natywnych plików binarnych cechowały się pewną niestabilnością.
Objawiała się ona wysokimi wartościami odchyleń standardowych (w przypadku GraalVM) lub nagłymi skokami wydajności wraz z zmianą pamięci (w przypadku Kotlin/Native).

Zatem dla funkcji AWS Lambda o wysokiej aktywności i wysokim ponownym użyciu rozgrzanych już instancji najbardziej uniwersalnym wyborem będzie użycie GraalVM.
Jednak w przypadku chęci użycia mniejszej pamięci, Kotlin/Native będzie sposobem bardziej stabilnym (ze względu na niższe odchylenia standardowe), który zapewni pewną poprawę czasu działania.
Dodatkowo, metoda ta wykazuje bardzo wysoką poprawę efektywności kosztowej.
Dla rozmiarów pamięci 128 MB i 256 MB Kotlin/Native pozwolił na uzyskanie najniższych kosztów ze wszystkich badanych metod, zatem może to być istotny czynnik w wyborze metod optymalizacji przez zespoły programistyczne.

Analizując przypadek funkcji mniej aktywnych (czyli z większym udziałem zimnych startów), metody wykazywały inne właściwości niż dla ciepłych startów.
Poprzez ocenę współczynnika wydajności funkcji (WWF) można stwierdzić, że większość metod pozwoliło na pewną poprawę wydajności.
Jedynie dla mniejszych pamięci (128 MB, 256 MB) metody jak SnapStart i Kotlin prowadziły do pogorszenia wydajności (oprócz użycia Javy z aktywnym SnapStart i pamięci 256 MB).
Dla pamięci wielkości 128 MB, 256 MB ponownie najbardziej skuteczne okazały się metody oparte o kompilację do natywnych plików binarnych.
Szczególną uwagę przykuwa jednak użycie Kotlin/Native, który zapewnił znacznie większy wzrost wydajności mierzony przez WWF niż GraalVM.
Wykazał się on najbardziej efektywną metodą dla funkcji o pamięci 128 MB, a także dla rozmiaru 256 MB, gdzie wzrost wydajności był kolosalny.
Kotlin/Native osiągnał trzykrotnie większą wartość współczynnika niż GraalVM, który był drugą najbardziej skuteczną metodą.
Czas uruchomienia funkcji dla pamięci 128 MB i 256 MB był także najniższy dla Kotlin/Native.
Zostanie to omówione szczegółowo w ramach odpowiedzi na drugie pytanie badawcze.
Dodatkowo, w ramach tych wartości pamięci możliwe było uzyskanie najniższych kosztów działania funkcji wśród wszystkich badanych konfiguracji.
Zostało to otrzymane dzięki użyciu kompilacji Kotlina do natywnych plików binarnych.
Wszystkie te aspekty wskazują na bardzo wysoką skuteczność tej metody w zakresie niewielkich rozmiarów pamięci (128 MB, 256 MB).

Dla mniej aktywnych funkcji o rozmiarze pamięci od 512 MB do 2048 MB, trzy z badanych metod wykazały istotną skuteczność.
Były to Kotlin/JS, Kotlin/Native oraz GraalVM.
Dla pamięci 512 MB, metody wykazały zbliżoną skuteczność pod względem współczynnika wydajności funkcji (przy czym GraalVM był najbardziej wydajny).
Wraz z wzrostem pamięci rosła wydajność wszystkich trzech metod, jednak istotny jest tutaj wzrost wydajności dla funkcji używających Kotlin/Native.
Dla rozmiaru pamięci 1024 MB Kotlin/Native uzyskał najwyższą wartość współczynnika, o około 10\% wyższą niż kolejny GraalVM.
Jednak dla rozmiaru 2048 MB, różnica ta zdecydowanie zwiększyła się, gdzie Kotlin/Native osiągnął wynik o około 74\% wyższy od GraalVM.
Wartym uwagi jest także bardzo wysoka wydajność Kotlin/Native w konfiguracji pamięci 256 MB, gdzie było to połączenie o drugiej najwyższej wartości współczynnika.
Dodatkowo, Kotlin/Native cechował się wysoką efektywnością kosztową, gdzie pozwolił na uzyskanie niższego kosztu działania niż GraalVM w prawie wszystkich z badanych rozmiarów pamięci (oprócz 512 MB).

Podsumowując, niemożliwe jest wybranie jednej najbardziej efektywnej metody optymalizacji wydajności dla wszystkich przypadków użycia funkcji AWS Lambda w ekosystemie Java.
Dla funkcji o bardzo wysokiej aktywności, GraalVM byłby najbardziej uniwersalną techniką, która jednak nie jest skuteczna dla każdej konfiguracji pamięci.
Podczas użycia funkcji o mniejszej aktywności i wyższym znaczeniu zimnych startów, bardzo sprawną techniką jest Kotlin/Native, który cechuje się także dobrą efektywnością kosztową, w porównaniu z alternatywnym GraalVM.
Wartym uwagi jest jednak przypadek funkcji o niewielkim rozmiarze pamięci (128 MB, 256 MB) i używających Kotlin/Native.
W ramach tych funkcji metoda ta pozwala na poprawę wydajności w ramach większości z badanych aspektów.
Osiąga ona najniższy czas działania dla zimnych startów oraz najwyższy współczynnik wydajności funkcji w ramach tych rozmiarów pamięci.
W przypadku ciepłych startów, jej efektywność jest wyższa lub podobna w porównaniu z funkcją Java JVM.
Wszystko to jest możliwe przy jednocześnie najniższych z otrzymanych w badaniu kosztów działania.
Z tego względu Kotlin/Native powinien być rozważany w przypadku decyzji o wyborze metody optymalizacji wydajności.

\subsection*{Odpowiedź na drugie pytanie badawcze}

Drugim pytaniem badawczym sformułowane w pracy jest: ,,W jakim stopniu wybrane metody optymalizacji redukują czas zimnego startu funkcji Java w AWS Lambda?''.
W celu udzielenia odpowiedzi w badaniu zawarto pomiar czasu działania funkcji podczas zimnych startów.
Dokładne wyniki zostały przedstawione w Rozdziale \ref{chapter:results_cold_start}.
W tym przypadku skuteczność metod była generalnie wysoka.
Jedynym wyjątkiem było zastosowanie funkcji SnapStart w połączeniu z Kotliniem, co wpłynęło negatywnie na efektywność podczas zimnych startów.
Dla Javy użycie SnapStart zredukowało czas zimnych startów od 10\% (dla pamięci 128 MB) do 55\% (dla pamięci 1024 MB).
Użycie Kotlina także było skuteczne, choć tylko dla większych rozmiarów pamięci (512 MB, 1024 MB, 2048 MB), gdzie poprawa wynosiła odpowiednio 28\%, 54\% oraz 65\%.

Najbardziej wydajnymi metodami były te oparte o kompilacje do natywnych plików binarnych (GraalVM, Kotlin/Native) oraz używające języków interpretowanych (Kotlin/JS).
Porównując GraalVM oraz Kotlin/JS, pierwsza metoda była średnio bardziej skuteczna dla małych pamięci (128 MB, 256 MB), jednak nie była to znaczna różnica.
Zapewniła ona poprawę średniego czasu odpowiedzi o odpowiednio 78\% i 81\%, gdy dla Kotlin/JS wynik ten wynosił odpowiednio 75\% i 80\%.
W przypadku pamięci 512 MB, 1024 MB i 2048 MB GraalVM zredukował czas odpowiedzi o odpowiednio 82\%, 82\% i 80\%, a Kotlin/JS zredukował czas odpowiedzi o odpowiednio 84\%, 84\% i 86\%.
Na bazie testów statystycznych wykazano, że dla pamięci 1024 MB przewaga Kotlin/JS jest znaczna, dla pozostałych już mniejsza.

Najefektywniejszą metodą okazał się jednak Kotlin/Native.
Dla każdej z badanych wielkości pamięci pozwolił on na największą redukcję czasu działania.
Wynosiła ona od 88\% (dla pamięci 128 MB) do 93\% (dla pamięci 1024 MB i 2048 MB) w porównaniu z bazową funkcją Java JVM.
Dla rozmiarów pamięci 128 MB i 256 MB, funkcje z Kotlin/Native charakteryzowały się zimnymi startami odpowiednio o około 45\% i 51\% niższymi w porównaniu do GraalVM.
Z kolei dla konfiguracji z pamięcią 512 MB, 1024 MB i 2048 MB, czasy zimnego startu były odpowiednio o 50\%, 48\% i 48\% krótsze niż dla Kotlin/JS.

\subsection*{Odpowiedź na trzecie pytanie badawcze}

Trzecim pytaniem badawczym, które zostało uwzględnione w pracy, jest: ,,Jakie kompromisy w procesie rozwoju oprogramowania wiążą się implementacją poszczególnych metod optymalizacji wydajności funkcji Java w AWS Lambda?''.
Wpływ na rozwój oprogramowania to istotny czynnik, który powinien być wzięty pod uwagę przez zespoły programistyczne podczas wyboru odpowiedniej metody optymalizacji wydajności.
Znaczenie może zostać ocenione pod kątem różnych kryteriów jak: czas budowy artefaktu, jego wielkość, dostępność narzędzi SDK (ang. Software Development Kit) czy kosztem działania.
W ramach oceny wszystkie metody są porównywane do bazowej implementacji Java działającej z użyciem JVM.

Podczas użycia usługi SnapStart programista dalej wykorzystuje ten sam artefakt i platformę, jak w funkcji bazowej.
Zatem nie ma ona wpływu na czynniki jak czas budowy, wielkość artefaktu czy dostępność SDK.
Istotny jest jednak wpływ na koszt funkcji, który był wyższy dla każdego z badanych rozmiarów pamięci oraz dla obu języków (Java i Kotlin).
W trakcie tworzenia systemów korzystających z SnapStart programista powinien uwzględnić jednak kilka aspektów związanych z stanem funkcji (co zostało opisane w Rozdziale \ref{chapter:snapstart}).
Wpływa to na połączenia sieciowe funkcji (szczególnie ważne podczas integracji z innymi serwisami) oraz generowania wartości unikalnych (jak identyfikatory).
Wynika to z sposobu działania SnapStart, który tworzy migawki stanu funkcji, wykorzystywane później w wielu wywołaniach.

Użycie GraalVM niesie za sobą pewne kompromisy dla rozwoju oprogramowania, które są jednak umiarkowane.
Istotny jest wpływ na tworzony artefakt funkcji, którego budowa znacznie wydłuża się (w porównaniu z tworzeniem klasycznego pliku JAR), a rozmiar zwiększa się.
GraalVM poprzez zmianę w sposobie kompilacji zmienia także zachowanie tworzonego kodu.
Wpływa to na np. mechanizm refleksji, którego użycie wymaga od programisty dodatkowej pracy związanej z odpowiednim oznaczenim metod i klas.
Możliwe jest jednak dalej użycie AWS SDK, by integrować funkcje z innymi usługami.
Mimo poprawy wydajności kosztów, ważnym aspektem jest jednak wzrost kosztów dla większych pamięci funkcji (1024 MB, 2048 MB).

Poprzez użycie języka Kotlin programista zyskusje szeroke możliwości związane z projektem Kotlin Multiplatform.
Język ten jest zbliżony do Javy i w przypadku użycia razem z JVM, nie niesie za sobą znacznych kompromisów w procesie rozwoju oprogramowania.
Należy jedynie wspomnieć o negatywnym wpływie na koszt działania, który średnio był wyższy niż dla analogicznych funkcji w Javie.
Większy wpływ na tworzenie aplikacji mają metody oparte o Kotlin/JS oraz Kotlin/Native.
Pierwszym z nich jest czas tworzenia artefaktu, który jest wyższy dla obu metod (jednak znacznie wyższy dla Kotlin/Native).
Warto jednak zaznaczyć, że rozmiar artefaktu był mniejszy dla wszystkich funkcji opartych o Kotlin, w porównaniu z klasyczną Javą.
Pewnym kompromisem są także integracje z zewnętrznymi serwisami, które mogą być utrudnione ze względu na dostępność AWS SDK.
Jest ono niedostępne dla Kotlin/Native oraz dostępne pośrednio dla Kotlin/JS, gdzie wymaga dodatkowej integracji.
Ograniczeniami dla tych sposobów są także ograniczone możliwości użycia mechanizmu refleksji, podobnie jak w przypadku GraalVM.

\section{Zagrożenia trafności wyników}\label{chapter:zagrozenia_trafnosci_wynikow}

\subsection*{Trafność wnioskowania}

Istotnym zagrożeniem dla wniosków wyciągniętych na podstawie eksperymentu może być niska moc statystyczna wyników. 
Badanie opierało się na dużej, jednak określonej, liczbie wywołań dla każdej z funkcji i konfiguracji. 
Mimo przeprowadzenia wielu testów, zmienność środowiska chmurowego i potencjalne losowe zmiany w czasach odpowiedzi mogły wpłynąć na wyniki. 
W ramach badania starano się zminimalizować ten wpływ, poprzez kilka czynników. 
Był to brak integracji z zewnętrznymi serwisami (np. API Gateway, DynamoDB), przeprowadzenie analizy statystycznej oraz wydłużenie czasu eksperymentu poprzez oczekiwanie pomiędzy kolejnymi wywołaniami.
Dodatkowo, eksperyment został wykonany w pojedynczym regione (eu-central-1) oraz w godzinach nocnych, gdzie obciążenie infrastruktury AWS jest mniejsze \cite{10.1145/3491204.3543506}.
Aby jeszcze bardziej zmniejszyć to ryzyko możliwe byłoby jeszcze większe wydłużenie czasu eksperymentu i regularne wywoływanie funkcji przez na przykład kilka dni.
Proces ten wymaga jednak znacznie dłuższego badania, które przekracza zakres aktualnej pracy.

\subsection*{Trafność wewnętrzna}

Potencjalnym czynnikiem, który mógłby negatywnie wpłynąć na trafność wewnętrzną eksperymentu jest dynamiczna natura środowiska chmurowego.
Inne procesy działające w tym samym czasie w infrastrukturze AWS i niezwiązane z badaniem, mogły teoretycznie wpłynąć na wydajność badanych funkcji (np. poprzez chwilowe obciążenie sieci lub współdzielonych zasobów).

Innym zagrożeniem dla funkcji opartych o maszynę wirtualnych o maszynę wirtualną Javy, może być mechanizm odśmiecania pamięci (Garbage Collector).
Z natury działa on niedeterministycznie, co może prowadzić do jego uruchomienia w ramach niektórych wywołań usługi.
Działanie to może mieć wpływ na ostateczne wyniki analizy statystycznej.
W ramach badania starano się zminimalizować jego wpływ poprzez wykonanie dużej liczby pomiarów.
Istotny jest jednak fakt, że mechanizm odśmiecania pamięci jest integralną częścią środowiska JVM.
W przypadku praktycznych zastosowań funkcji opartych o to środowisko, wpływ odśmiecania pamięci prawdopodobnie nie byłby kontrolowany.
Z tego względu, przy jednoczesnym zachowaniu wielokrotnych wywołań, jego losowy wpływ może w pewnym stopniu lepiej odzwierciedlać praktyczną wydajność funkcji.

Innym aspektem są potencjalne błędy lub różnice w implementacji funkcji.
W przypadku języka Kotlin postarano się zminimalizować ten wpływ poprzez współdzielenie kodu między wszystkiem metody oparte o ten język.
Jednak poszczególne metody stosują różne biblioteki i frameworki, których wybór jest ograniczony poprzez daną metodę (np. wsparcie dla kompilacji natywnej).
Sama ich analiza i porównanie może być jednak wymagające, co stanowi potencjalny kierunek rozwoju pracy.

\subsection*{Trafność konstruktu}

Trafność konstruktu mogła być zagrożona ze poprzez przyjęte w pracy miary wydajności oraz kompromisów w procesiu rozwoju oprogramowania.
Wydajność została zmierzona za pomocą czasu wykonania (ciepłe i zimne starty), kosztu oraz autorskiego współczynnika WWF.
Trafność ta może być jednak zagrożona ze względu na zróżnicowane proporcje ciepłych i zimnych startów w systemach opartych o funkcje AWS Lambda.
W ramach badania przyjęto jedynie pojedyncze ich proporcje, zatem istnieje ryzyko niezawarcia w badaniu szczególnych przypadków użycia badanej usługi.

Wpływ na proces rozwoju oprogramowania oceniono na podstawie czasu budowy artefaktu, jego wielkości, dostępności AWS SDK, kosztu działania funkcji oraz innych czynników opisanych w Rozdziale \ref{chapter:wybrane_metody_optimalizacji}.
Opis procesu rozwoju oprogramowania jedynie w tych aspektach jest jednak uproszczony.
Niesie to zatem ryzyko nieprawidłowej oceny konkretnych metod.
Aby zapewnić lepszą jakość badania pod tym względem, możliwa byłaby analiza konkretnych projektów korzystających z danych metod.
Jest to jednak utrudnione ze względu na niewielką popularność części z nich.

\subsection*{Trafność zewnętrzna}

W badaniu zaimplementowano bardzo ogólne zadanie obliczeniowe czyli mnożenie macierzy oraz powstrzymano się od integracji zewnętrznych usług.
Miało to na celu ograniczenie wpływu zewnętrznych czynników na czas działania funkcji.
Jak wykazano w przeglądzie literatury dodatkowe serwisy są jednak bardzo częstym elementem architektur bezserwerowych.
Z tego powodu zaimplementowane operacje mogą być niewystarczające aby przełożyć wyniki badania na przypadki innych systemów.
Systemy te mogą opierać się np. o wywołania dodatkowych API, które nie zostały uwzględnione w eksperymencie.

AWS Lambda zapewnia istotną abstrakcję na infrastrukturę chmurową, która jednak dalej odgrywa istotną rolę w wydajności funkcji.
Eksperyment przeprowadzono w jednym regionie AWS i dla jednej architektury procesora (x86-64). 
Jak wykazał przegląd literatury, wydajność może różnić się w zależności od regionu geograficznego oraz architektury (np. ARM64). 
Zatem wyniki mogą nie być w pełni reprezentatywne dla wszystkich dostępnych konfiguracji.

\section{Nowe kierunki rozwoju i zaawansowane techniki}

Poprawa wydajności jest widocznym trendem zarówno w usłudze AWS Lambda, jak i narzędziach ekosystemu Java.
Prowadzi to do możliwości wykorzystania nowych technologii w celu dalszej poprawy efektywności tworzonych funkcji.
W ramach rozdziału opisano techniki, które mogą stanowić rozwinięcie badań przedstawionych w pracy.
Opisane sposoby nie zostały uwzględnione w pierwotnym opracowaniu. 
Ze względu na swoją użyteczność mogą jednak stanowić cenne uzupełnienie istniejących metod oraz wyznaczać kierunki dalszych analiz.

\subsection*{Ładowanie i łączenie klas z wyprzedzeniem}

Ładowanie i łączenie klas z wyprzedzeniem (ang. Ahead-of-Time Class Loading and Linking) to nowoczesna technika, która jest rozwijana w ramach OpenJDK.
Została zaprezentowana w JEP 483 (ang. JDK Enhancement Proposal) oraz zawartym w najnowszej wersji Java 24 \cite{JEP483}, która osiągnęła ogólną dostępność w marcu 2025 roku.
Głównym celem inicjatywy zawartej w JEP 483 jest optymalizacja czasu uruchomienia aplikacji oraz redukcja zużycia pamięci.
Element ten jest kluczowy w środowiskach chmurowych, a tym bardziej bezserwerowych.
Mechanizm ten jest częścią szerszej inicjatywy ,,Project Leyden'', co stanowi ewolucję istniejącej funkcji współdzielenia danych klas (ang. Class-Data Sharing).

Omawiana technika opiera się na przygotowaniu pamięci podręcznej AOT (ang. Ahead-of-Time). 
Jest ona tworzona przed właściwym uruchomieniem, na podstawie trybu treningowego.
Podczas treningowego uruchomienia, mechanizm identyfikuje klasy ładowane przy starcie aplikacji, a następnie zapisuje ich załadowane i połączone dane w pliku.
Dzięki temu, przy kolejnych uruchomieniach, maszyna wirtualna wykorzystuje ten gotowy artefakt.
Znacząco przyspiesza to start maszyny wirtualnej oraz nie wymaga przy tym żadnych zmian w kodzie źródłowym.

Znaczenie tej technologii dla całego ekosystemu Java podkreśla zainteresowanie nią ze strony społeczności frameworku Spring.
Zespół Spring postrzega JEP 483 jako obiecującą metodę na znaczące skrócenie czasu uruchomienia, bez konieczności sięgania po kompilację natywną i rezygnacji z dynamicznych mechanizmów refleksji, co jest jednym z głównych kompromisów przy użyciu GraalVM.
Wsparcie dla tej funkcji \cite{spring-aot-docs} jest zatem sygnałem, który potwierdza istotę JEP483 w ekosystemie Java.
Poprawa wydajności z jej użyciem jest istotna, gdyż jak wskazują eksperymenty, pozwala na poprawę czasu startu aplikacji o około 42\% \cite{JEP483}.
Pomiar został przeprowadzony na aplikacji Spring PetClinic, przez twórców funkcji.

Przedstawiona technika stanowi interesujące rozwinięcie metod optymalizacji wydajności opartych o standardową maszynę wirtualną Javy.
Może być ona wartą uwagi alternatywą dla kompilacji natywnych, która nie ogranicza możliwości oferowanych przez język programowania.
Jej potencjał może leżeć także w połączeniu z innymi mechanizmami AWS Lambda, jak SnapStart.
Przyspieszenie fazy inicjalizacji maszyny wirtualnej przez AOT Cache mogłoby bezpośrednio skrócić czas potrzebny na utworzenie migawki przez SnapStart, co prowadziłoby do szybszego wdrażania nowych wersji funkcji.
Oba te obszary wyznaczają nowy, interesujący kierunek badań nad dalszą poprawą wydajności funkcji AWS Lambda.

\subsection*{AWS Graviton}

Innym interesującym kierunkiem rozwoju jest wykorzystanie architektur ARM, które zyskują na popularności w centrach danych dzięki swojej efektywności.
Kluczową rolę w kontekście chmury obliczeniowej firmy Amazon odgrywają oferowane przez nią procesory AWS Graviton.
Najnowszy z nich, czyli Graviton4 został zaprezentowany przez Amazon w lipcu 2024 roku.
Wybór tego rodzaju procesorów pozwala na znaczną poprawę wydajności w stosunku do kosztów działania funkcji.

W kontekście tej metody, szczególnego znaczenia nabierają techniki oparte o kompilację natywną Kotlin/Native.
Umożliwia ona kompilację kodu Kotlin bezpośrednio do binarnego pliku wykonywalnego dla architektury linuxArm64 \cite{kotlinlangKotlinDocs}.
W ramach pracy wykazano istotne zdolności Kotlin/Native w kontekście optymalizacji kosztowej.
W przypadku użycia z procesorami Graviton potencjał ten może wzrosnąć, a co za tym idzie pozwolić na jeszcze większą wydajność funkcji przy niższych kosztach.
Będzie to możliwe dzięki redukcji czasu zimnych startów przez Kotlin/Native i wysoką efektywność architektury ARM.
Dodatkowo, skuteczność Kotlin/Native w kontekście małych pamięci może otworzyć nową drogą do użycia niewielkich i wyspecjalizowanych funkcji AWS Lambda.
Będą one cechować się minimalnym kosztem utrzymania i wysoką wydajnością. 
Pozwoli to na budowanie kompletnych systemów z drobnych, wyspecjalizowanych zadań. 
Stanowi to silną alternatywę dla funkcji pisanych w językach takich jak JavaScript lub Python, co pozwoli zespołom programistycznym korzystanie z spójnego ekosystemu opartego o język Kotlin.

\subsection*{Rozwój Kotlin Multiplatform}

Popularność projektu Kotlin Multiplatform stale rośnie, czego dowodem jest rosnąca liczba bibliotek integrujących różne platformy przez niego oferowane.
Mogą być one wyszukane i analizowane poprzez użycie specjalnej wyszukiwarki ,,klibs.io'', stworzonej przez JetBrains, czyli twórców język Kotlin.
Rozwój ten jest także widoczny poprzez osiągnięcie przez projekt stabilności, co czyni go w pełni gotowym do zastosowań produkcyjnych, także po stronie serwera.
Istotnym czynnikiem w tym rozwoju jest wsparcie frameworków dla języka Kotlin, takich jak Spring Boot \cite{spring-kotlin-docs}.
Twórcy narzędzia intensywnie inwestują w integracje języka, co pozwala na osiągnięcie lepszej satysfakcji programistów rozwijających systemy w tej technologii.

Jedną z kluczowych czynności wykonywanych podczas rozwoju funkcji AWS Lambda jest dobór odpowiedniego rozmiaru pamięci.
Jest to dokonywane także w trakcie życia fukcji, której pamięć jest stale optymalizowana pod kątem rozmiaru.
Kotlin Multiplatform pozwala jednak pójść o krok dalej dostarczając możliwość optymalizacji docelowego środowiska wykonawczego.
Zamiast jedynie dostrajać parametry dla jednego środowiska (np. JVM), zespoły mogą wybrać różne środowiska dla każdej funkcji z osobna, w zależności od jej zadań.
Poprzez współdzielenie kodu możliwe byłaby dynamiczna zmiana tego środowiska, gdzie ten sam kod mógłby zostać skompilowany do różnych platform.

To podejście otwiera nową drogę do testów wykonywanych na funkcjach AWS Lambda.
Poprzez mechanizmy routingu na poziomie usług takich jak API Gateway, możliwe jest wdrożenie kilku wariantów tej samej logiki biznesowej jako oddzielne funkcje Lambda (np. w wersji JVM, JS i natywnej).
Dzięki temu poprzez odpowiedni podział ruchu, możliwe jest monitorowanie wydajności poszczególnych funkcji w czasie rzeczywistym.
Na bazie tego możliwa jest decyzja odnośnie ostatecznie wybranego środowiska wykonawczego usługi.
Co ważne, wybór ten nie wymaga dodatkowych nakładów pracy, które byłyby konieczne w przypadku używania róznych języków (np. Java, Python i JavaScript) w ramach pojedynczego systemu.
Jest to możliwe dzięki funkcji wspołdzielenie kodu przez Kotlin Multiplatform.

\subsection*{Projekt CRaC}

Kolejną kluczową inicjatywą w kontekście zimnych startów Javy jest projekt CRaC, czyli skoordynowane przywracanie w punkcie kontrolnym (ang. Coordinated Restore at Checkpoint).
Jest to projekt rozwijany w ramach OpenJDK, którego celem jest drastyczne skrócenie czasu uruchamiania aplikacji Java.
Mechanizm ten opiera się na tworzeniu migawek oraz późniejszym przywracaniu ich.
Pozwala to na uruchomienie aplikacji w momencie, gdy wszystkie wymagane klasy są już rozgrzane, co pozwala na szybszą inicjalizację programu.
Istotna jest ,,koordynacja'' tej funkcji, która oznacza, że programista jest w stanie implementować odpowiednie funkcje, które są uruchamiane w momencie konkretnych zdarzeń (np. przed wykonaniem migawki).
Dzięki temu aplikacja aktywnie uczestniczy w procesie tworzenia migawek oraz go koordynuje.

Dalszy rozwój projektu CRaC w ramach społeczności OpenJDK niesie ze sobą obiecujące perspektywy dla użytkowników SnapStart. 
Wszelkie usprawnienia w mechanizmie tworzenia punktów kontrolnych mogą w przyszłości zostać zaimplementowane w usłudze AWS, czyniąc ją jeszcze bardziej efektywną.
Badanie możliwości oraz wpływu CRaC na usługę SnapStart i bezpośrednio funkcje AWS Lambda stanowi interesujący obszar dalszych badań.
Ma to szczególne znaczenie w kontekście języka Kotlin, dla którego, jak wykazano w pracy, obecna implementacja SnapStart jest mało skuteczna, a usprawnienia w CRaC mogłyby rozwiązać ten problem.

\subsection*{Wsparcie zaawansowanych frameworków}

Dużą popularnością wśród programistów cechują się różnego rodzaju frameworki ekosystemu Java.
Przeszły one jednak transformację, gdzie przestały być skupione wyłącznie na tworzeniu serwerów, ale coraz mocniej wspierają integracje z funkcjami bezserwerowymi i ich optymalizację.
Ich główną rolą w architekturze serverless jest abstrakcja i uproszczenie implementacji zaawansowanych technik, takich jak kompilacja do obrazów natywnych czy integracja z usługą AWS SnapStart.
Z tego powodu, wybór dedykowanego frameworka jest ważną decyzją architektoniczną.
Wpływa ona bezpośrednio na wydajność oraz jakość rozwoju oprogramowania.

Trend użycia frameworków w AWS Lambda podkreśla technologia AWS Lambda Web Adapter.
Umożliwia ona uruchamianie standardowych aplikacji webowych jako funkcji AWS Lambda bez modyfikacji kodu, co znacznie wspiera migrację istniejących systemów do usług bezserwerowych.
Spring Boot w wersji 3, dzięki wsparciu dla kompilacji natywnej, pozwala na transformację tradycyjnych monolitów w wysoce zoptymalizowane funkcje \cite{spring_cloud_function_2025}.
Micronaut poprzez architekturę opartą o kompilację Ahead-of-Time (AOT) oferuje natychmiastową gotowość do pracy w wysoce wydajnym środowisku funkcji AWS Lambda \cite{micronaut_docs_2025}.
Quarkus z kolei, zaprojektowany dla chmury, przenosi znaczną część logiki startowej do fazy budowania \cite{quarkus-docs}.
Wszystkie te narzędzia wspierają dodatkowo kompilację natywną z użyciem GraalVM, co podkreśla rozbudowany trend poprawy wydajności.

Z tego powodu, wybór odpowiedniego frameworka sam w sobie jest zaawansowaną techniką i istotną decyzją architektoniczną. 
Analiza tych narzędzi pod kątem wydajności oraz ich wpływu na jakość rozwoju oprogramowania wyznacza interesujący kierunek do dalszych badań.
Może to być naturalne rozwinięcie badań zawartych już w pracy.
Pozwoli to na dokładną ocenę zachowania opisanych narzędzi zarówno z językiem Java, jak i Kotlin oraz z technikami GraalVM i SnapStart.
Dzięki temu zespoły programistyczne będą w stanie podjąć bardziej świadomą decyzję o wyborze frameworka dla AWS Lambda.

\subsection*{Wysoce wydajne środowiska wykonawcze JavaScript}

Trend poprawy wydajności widoczny jest nie tylko w ekosystemie Java, ale także w ramach języka JavaScript.
W ostatnim czasie obserwujemy rozwój coraz to nowszych środowisk wykonawczych, które są alternatywą dla tradycyjnego Node.js.
Są to na przykład Deno, Bun czy LLRT (ang. Low-Latency Runtime), które skupiają się na poprawie efektywności wykonywania kodu JavaScript.
Środowisko Deno oparte jest o język Rust, co podkreśla jego dążenia do osiągnięcia wysokiej efektywności.
Natomiast Bun jest projektem skupionym na osiągnięciu jak najkrótszych czasów wykonania, poprzez użycie języka Zig i silnika JavaScriptCore.
Na szczególną uwagę zasługuje LLRT, który rozwijany jest bezpośrednio przez AWS i wyspecjalizowany do użycia w funkcjach AWS Lambda.
W porównaniu z Node.js, pozwala on na znaczną redukcję czasów startu.
Dla 95. percentyla wywołań, czasy zimnych i ciepłych startów są krótsze o odpowiednio około 86\% i 69\% \cite{llrt2025}.
Samo istnienie LLRT jest sygnałem o wysokiej motywacji społeczności JavaScript do projektowania rozwiązań skupionych na efektywności, co jest kluczowe dla AWS Lambda.

Pojawienie się wspomnianych środowisk tworzy zupełnie nową ścieżkę rozwoju dla Kotlin/JS.
Możliwość translacji kodu Kotlin do JavaScript oznacza, że jego wykonanie nie jest ograniczone tylko do środowiska Node.js.
Kompletnie nowym obszarem badań mogą być procesy i mechanizmy, które pozwolą na wykonanie kodu stworzonego przez Kotlin/JS w Deno, Bun lub LLRT.
Opracowanie takich rozwiązań byłoby istotnym krokiem naprzód, pozwalającym łączyć zalety języka Kotlin z wysoką wydajnością oferowaną przez wyspecjalizowane środowiska wykonawcze.
